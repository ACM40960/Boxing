{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b75008",
   "metadata": {},
   "source": [
    "#### Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c5674",
   "metadata": {},
   "source": [
    "##### Punches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224e88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¥ IP Webcam Recorder (Preview Always On)\n",
      "  a â†’ jab / orthodox / left\n",
      "  b â†’ jab / orthodox / right\n",
      "  c â†’ jab / southpaw / left\n",
      "  d â†’ jab / southpaw / right\n",
      "  e â†’ cross / orthodox / left\n",
      "  f â†’ cross / orthodox / right\n",
      "  g â†’ cross / southpaw / left\n",
      "  h â†’ cross / southpaw / right\n",
      "  i â†’ hook / orthodox / left\n",
      "  j â†’ hook / orthodox / right\n",
      "  k â†’ hook / southpaw / left\n",
      "  l â†’ hook / southpaw / right\n",
      "  m â†’ uppercut / orthodox / left\n",
      "  n â†’ uppercut / orthodox / right\n",
      "  o â†’ uppercut / southpaw / left\n",
      "  p â†’ uppercut / southpaw / right\n",
      "  z â†’ negative / orthodox / left\n",
      "  y â†’ negative / southpaw / right\n",
      "  s â†’ negative / orthodox / right\n",
      "  t â†’ negative / southpaw / left\n",
      "  1 â†’ block / orthodox / right\n",
      "  2 â†’ block / southpaw / left\n",
      "  3 â†’ block / orthodox / left\n",
      "  4 â†’ block / southpaw / right\n",
      "Press 'q' to quit.\n",
      "\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 6\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 1\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x206bb0c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n",
      "\n",
      "ðŸŽ¬ Recording CROSS (orthodox, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/cross\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x25d0e240] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20049dc0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 6\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20650d40] overread 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #040\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (orthodox, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n",
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x202ffb00] overread 8\n",
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20049cc0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n",
      "\n",
      "ðŸŽ¬ Recording NEGATIVE (southpaw, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: boxing_dataset/raw/negative\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x20dfa640] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”š Exiting.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_DIR = \"punch/combined_dataset\"\n",
    "LABELS_FILE = \"labels/punch_label.csv\"\n",
    "PREVIEW_WIDTH = 360\n",
    "PREVIEW_HEIGHT = 640\n",
    "FULL_HD = (1080, 1920)\n",
    "FPS = 20\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "\n",
    "STREAM_URLS = {\n",
    "    \"cam 1\": \"http://192.168.39.134:8080/video\",\n",
    "    \"cam 2\": \"http://10.128.92.136:8080/video\",\n",
    "    \"cam 3\": \"http://192.168.67.89:8080/video\",\n",
    "    \"cam 4\": \"http://10.18.32.16:8080/video\",\n",
    "}\n",
    "\n",
    "key_map = {\n",
    "    'a': ('jab', 'orthodox', 'left'),\n",
    "    'b': ('jab', 'orthodox', 'right'),\n",
    "    'c': ('jab', 'southpaw', 'left'),\n",
    "    'd': ('jab', 'southpaw', 'right'),\n",
    "    'e': ('cross', 'orthodox', 'left'),\n",
    "    'f': ('cross', 'orthodox', 'right'),\n",
    "    'g': ('cross', 'southpaw', 'left'),\n",
    "    'h': ('cross', 'southpaw', 'right'),\n",
    "    'i': ('hook', 'orthodox', 'left'),\n",
    "    'j': ('hook', 'orthodox', 'right'),\n",
    "    'k': ('hook', 'southpaw', 'left'),\n",
    "    'l': ('hook', 'southpaw', 'right'),\n",
    "    'm': ('uppercut', 'orthodox', 'left'),\n",
    "    'n': ('uppercut', 'orthodox', 'right'),\n",
    "    'o': ('uppercut', 'southpaw', 'left'),\n",
    "    'p': ('uppercut', 'southpaw', 'right'),\n",
    "    'z': ('negative', 'orthodox', 'left'),\n",
    "    'y': ('negative', 'southpaw', 'right'),\n",
    "    's': ('negative', 'orthodox', 'right'),\n",
    "    't': ('negative', 'southpaw', 'left'),\n",
    "}\n",
    "\n",
    "def find_next_sample_number(label, stance, hand):\n",
    "    subdir = os.path.join(BASE_DIR, label)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    used_numbers = set()\n",
    "\n",
    "    for fname in os.listdir(subdir):\n",
    "        if fname.startswith(f\"{label}_{stance}_{hand}_\") and fname.endswith('.mp4'):\n",
    "            try:\n",
    "                num_part = fname.split('_')[-1].split('.')[0]\n",
    "                num = int(num_part)\n",
    "                used_numbers.add(num)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    for i in range(1, 1000):  \n",
    "        if i not in used_numbers:\n",
    "            return f\"{i:03d}\"\n",
    "    return f\"{max(used_numbers)+1:03d}\"  \n",
    "\n",
    "def create_writer(filename):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    return cv2.VideoWriter(filename, fourcc, FPS, FULL_HD)\n",
    "\n",
    "def record_punch(label, stance, hand):\n",
    "    count_str = find_next_sample_number(label, stance, hand)\n",
    "\n",
    "    subdir = os.path.join(BASE_DIR, label)\n",
    "    caps = {name: cv2.VideoCapture(url) for name, url in STREAM_URLS.items()}\n",
    "    for name, cap in caps.items():\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open stream for {name}\")\n",
    "            return\n",
    "\n",
    "    writers = {}\n",
    "    out_paths = []\n",
    "    for name in STREAM_URLS:\n",
    "        fname = f\"{label}_{stance}_{hand}_{name}_{count_str}.mp4\"\n",
    "        path = os.path.join(subdir, fname)\n",
    "        writers[name] = create_writer(path)\n",
    "        out_paths.append(os.path.join(label, fname))\n",
    "\n",
    "    print(f\"\\nRecording {label.upper()} ({stance}, {hand}) â€” Sample #{count_str}\")\n",
    "\n",
    "    start = time.time()\n",
    "    while time.time() - start < RECORD_SECONDS:\n",
    "        for name, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                writers[name].write(frame)\n",
    "                preview_frame = cv2.resize(frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "                cv2.circle(preview_frame, (30, 30), 15, (0, 0, 255), -1)\n",
    "                cv2.imshow(name, preview_frame)\n",
    "            else:\n",
    "                print(f\"Failed to read from {name}\")\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Early stop requested.\")\n",
    "            break\n",
    "        time.sleep(1 / FPS)\n",
    "\n",
    "    for cap in caps.values(): cap.release()\n",
    "    for writer in writers.values(): writer.release()\n",
    "\n",
    "    with open(LABELS_FILE, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for fname in out_paths:\n",
    "            writer.writerow([fname, stance, hand, label])\n",
    "\n",
    "    print(f\"Saved {len(out_paths)} clips to: {subdir}\")\n",
    "\n",
    "def main():\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        with open(LABELS_FILE, 'w', newline='') as f:\n",
    "            csv.writer(f).writerow([\"filename\", \"stance\", \"hand_used\", \"label\"])\n",
    "\n",
    "    print(\"\\nIP Webcam Recorder (Preview Always On)\")\n",
    "    for k, (label, stance, hand) in key_map.items():\n",
    "        print(f\"  {k} â†’ {label} / {stance} / {hand}\")\n",
    "    print(\"Press 'q' to quit.\\n\")\n",
    "\n",
    "    caps = {name: cv2.VideoCapture(url) for name, url in STREAM_URLS.items()}\n",
    "    for name, cap in caps.items():\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open preview stream: {name}\")\n",
    "            return\n",
    "\n",
    "    while True:\n",
    "        for name, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                preview = cv2.resize(frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "                cv2.imshow(name, preview)\n",
    "            else:\n",
    "                print(f\"Lost feed from {name}\")\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        elif key != 255:\n",
    "            key_char = chr(key).lower()\n",
    "            if key_char in key_map:\n",
    "                label, stance, hand = key_map[key_char]\n",
    "                record_punch(label, stance, hand)\n",
    "\n",
    "    for cap in caps.values():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54c3f2",
   "metadata": {},
   "source": [
    "##### Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd40cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¥ IP Webcam Recorder (Preview Always On)\n",
      "  a â†’ parry / orthodox / left\n",
      "  b â†’ parry / orthodox / right\n",
      "  c â†’ parry / southpaw / left\n",
      "  d â†’ parry / southpaw / right\n",
      "  e â†’ high_guard / orthodox / left\n",
      "  f â†’ high_guard / orthodox / right\n",
      "  g â†’ high_guard / southpaw / left\n",
      "  h â†’ high_guard / southpaw / right\n",
      "  i â†’ elbow_block / orthodox / left\n",
      "  j â†’ elbow_block / orthodox / right\n",
      "  k â†’ elbow_block / southpaw / left\n",
      "  l â†’ elbow_block / southpaw / right\n",
      "  m â†’ forearm_block / orthodox / left\n",
      "  n â†’ forearm_block / orthodox / right\n",
      "  o â†’ forearm_block / southpaw / left\n",
      "  p â†’ forearm_block / southpaw / right\n",
      "Press 'q' to quit.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11e31d80] overread 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1121f100] overread 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x16753d40] overread 8\n",
      "[mjpeg @ 0x16753d40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11f6ff80] overread 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11549f80] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11f5aac0] overread 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (orthodox, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 7\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 3\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1235d480] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x115dcbc0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x115dcfc0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x122281c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12224c80] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x111a1780] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/parry\n",
      "\n",
      "ðŸŽ¬ Recording PARRY (southpaw, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/parry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 6\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 2\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12cc2f40] overread 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12228300] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12228500] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (orthodox, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 6\n",
      "[mjpeg @ 0x1258b280] overread 3\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1121b100] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 7\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11b9a480] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12225080] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x111a1780] overread 8\n",
      "[mjpeg @ 0x111a1780] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 4\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x112922c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11cf1940] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n",
      "\n",
      "ðŸŽ¬ Recording HIGH_GUARD (southpaw, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/high_guard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 5\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 3\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12228300] overread 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x122281c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1122f200] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x122281c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #006\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x125716c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x121ffec0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x121ff5c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12225200] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x111a1780] overread 8\n",
      "[mjpeg @ 0x111a1780] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (orthodox, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 7\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x11683f40] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n",
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #005\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11f70880] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x122283c0] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11716040] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x111a1780] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12228300] overread 8\n",
      "[mjpeg @ 0x12228300] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x12225200] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, left) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #001\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #002\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #003\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #004\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x121fa300] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1140a540] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #007\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x11683f40] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #008\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #009\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #010\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #011\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #012\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #013\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #014\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #015\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #016\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #017\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #018\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #019\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #020\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #021\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #022\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #023\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #024\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #025\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #026\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #027\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #028\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #029\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #030\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #031\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #032\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #033\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #034\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #035\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #036\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #037\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #038\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #039\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #040\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #041\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #042\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #043\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #044\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #045\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #046\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #047\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #048\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mjpeg @ 0x1258b280] overread 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #049\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "\n",
      "ðŸŽ¬ Recording FOREARM_BLOCK (southpaw, right) â€” Sample #050\n",
      "âœ… Saved 2 clips to: block_dataset/forearm_block\n",
      "ðŸ”š Exiting.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "BASE_DIR = \"block_negative/block_dataset\"\n",
    "LABELS_FILE = \"labels/block_labels.csv\"\n",
    "PREVIEW_WIDTH = 360\n",
    "PREVIEW_HEIGHT = 640\n",
    "FULL_HD = (1080, 1920)\n",
    "FPS = 20\n",
    "RECORD_SECONDS = 3\n",
    "\n",
    "\n",
    "STREAM_URLS = {\n",
    "    \"cam 1\": \"http://192.168.39.134:8080/video\",\n",
    "    \"cam 2\": \"http://10.128.92.136:8080/video\",\n",
    "    \"cam 3\": \"http://192.168.67.89:8080/video\",\n",
    "    \"cam 4\": \"http://10.18.32.16:8080/video\",\n",
    "}\n",
    "\n",
    "\n",
    "key_map = {\n",
    "    'a': ('parry', 'orthodox', 'left'),\n",
    "    'b': ('parry', 'orthodox', 'right'),\n",
    "    'c': ('parry', 'southpaw', 'left'),\n",
    "    'd': ('parry', 'southpaw', 'right'),\n",
    "\n",
    "    'e': ('high_guard', 'orthodox', 'left'),\n",
    "    'f': ('high_guard', 'orthodox', 'right'),\n",
    "    'g': ('high_guard', 'southpaw', 'left'),\n",
    "    'h': ('high_guard', 'southpaw', 'right'),\n",
    "\n",
    "    'i': ('elbow_block', 'orthodox', 'left'),\n",
    "    'j': ('elbow_block', 'orthodox', 'right'),\n",
    "    'k': ('elbow_block', 'southpaw', 'left'),\n",
    "    'l': ('elbow_block', 'southpaw', 'right'),\n",
    "\n",
    "    'm': ('forearm_block', 'orthodox', 'left'),\n",
    "    'n': ('forearm_block', 'orthodox', 'right'),\n",
    "    'o': ('forearm_block', 'southpaw', 'left'),\n",
    "    'p': ('forearm_block', 'southpaw', 'right'),\n",
    "}\n",
    "\n",
    "\n",
    "def find_next_sample_number(label, stance, hand):\n",
    "    subdir = os.path.join(BASE_DIR, label)\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "    used_numbers = set()\n",
    "\n",
    "    for fname in os.listdir(subdir):\n",
    "        if fname.startswith(f\"{label}_{stance}_{hand}_\") and fname.endswith('.mp4'):\n",
    "            try:\n",
    "                num_part = fname.split('_')[-1].split('.')[0]\n",
    "                num = int(num_part)\n",
    "                used_numbers.add(num)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    for i in range(1, 1000):  \n",
    "        if i not in used_numbers:\n",
    "            return f\"{i:03d}\"\n",
    "    return f\"{max(used_numbers)+1:03d}\"  \n",
    "\n",
    "def create_writer(filename):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    return cv2.VideoWriter(filename, fourcc, FPS, FULL_HD)\n",
    "\n",
    "def record_Block(label, stance, hand):\n",
    "    count_str = find_next_sample_number(label, stance, hand)\n",
    "\n",
    "    subdir = os.path.join(BASE_DIR, label)\n",
    "    caps = {name: cv2.VideoCapture(url) for name, url in STREAM_URLS.items()}\n",
    "    for name, cap in caps.items():\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open stream for {name}\")\n",
    "            return\n",
    "\n",
    "    writers = {}\n",
    "    out_paths = []\n",
    "    for name in STREAM_URLS:\n",
    "        fname = f\"{label}_{stance}_{hand}_{name}_{count_str}.mp4\"\n",
    "        path = os.path.join(subdir, fname)\n",
    "        writers[name] = create_writer(path)\n",
    "        out_paths.append(os.path.join(label, fname))\n",
    "\n",
    "    print(f\"\\nRecording {label.upper()} ({stance}, {hand}) â€” Sample #{count_str}\")\n",
    "\n",
    "    start = time.time()\n",
    "    while time.time() - start < RECORD_SECONDS:\n",
    "        for name, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                writers[name].write(frame)\n",
    "                preview_frame = cv2.resize(frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "                cv2.circle(preview_frame, (30, 30), 15, (0, 0, 255), -1)\n",
    "                cv2.imshow(name, preview_frame)\n",
    "            else:\n",
    "                print(f\"Failed to read from {name}\")\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Early stop requested.\")\n",
    "            break\n",
    "        time.sleep(1 / FPS)\n",
    "\n",
    "    for cap in caps.values(): cap.release()\n",
    "    for writer in writers.values(): writer.release()\n",
    "\n",
    "    with open(LABELS_FILE, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        for fname in out_paths:\n",
    "            writer.writerow([fname, stance, hand, label])\n",
    "\n",
    "    print(f\"Saved {len(out_paths)} clips to: {subdir}\")\n",
    "\n",
    "def main():\n",
    "    os.makedirs(BASE_DIR, exist_ok=True)\n",
    "    if not os.path.exists(LABELS_FILE):\n",
    "        with open(LABELS_FILE, 'w', newline='') as f:\n",
    "            csv.writer(f).writerow([\"filename\", \"stance\", \"hand_used\", \"label\"])\n",
    "\n",
    "    print(\"\\nIP Webcam Recorder (Preview Always On)\")\n",
    "    for k, (label, stance, hand) in key_map.items():\n",
    "        print(f\"  {k} â†’ {label} / {stance} / {hand}\")\n",
    "    print(\"Press 'q' to quit.\\n\")\n",
    "\n",
    "    caps = {name: cv2.VideoCapture(url) for name, url in STREAM_URLS.items()}\n",
    "    for name, cap in caps.items():\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Failed to open preview stream: {name}\")\n",
    "            return\n",
    "\n",
    "    while True:\n",
    "        for name, cap in caps.items():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                preview = cv2.resize(frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "                cv2.imshow(name, preview)\n",
    "            else:\n",
    "                print(f\"Lost feed from {name}\")\n",
    "\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            print(\"Exiting.\")\n",
    "            break\n",
    "        elif key != 255:\n",
    "            key_char = chr(key).lower()\n",
    "            if key_char in key_map:\n",
    "                label, stance, hand = key_map[key_char]\n",
    "                record_Block(label, stance, hand)\n",
    "\n",
    "    for cap in caps.values():\n",
    "        cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee85ca48",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566405bd",
   "metadata": {},
   "source": [
    "##### Punch Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4902bf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Video Augmentation Process ---\n",
      "\n",
      "Processing action: 'jab' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'jab': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:47:13<00:00,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'cross' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'cross': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:44:12<00:00,  7.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'hook' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'hook': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:43:53<00:00,  7.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'uppercut' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'uppercut': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:39:44<00:00,  7.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'negative' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'negative': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:39:17<00:00,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Finished augmenting all specified video folders.\n",
      "Augmented videos saved in: augmented_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_RAW_DATA_DIR = \"punch/combined_dataset\"\n",
    "BASE_AUGMENTED_DATA_DIR = \"punch/augmented_data\"\n",
    "\n",
    "ACTIONS_TO_PROCESS = [\n",
    "    'jab', 'cross', 'hook', 'uppercut']\n",
    "\n",
    "AUGMENTATION_TYPES = {\n",
    "    \"rot_pos15\": {\"angle\": 15},\n",
    "    \"rot_neg15\": {\"angle\": -15},\n",
    "    \"scale_80\": {\"scale\": 0.8},\n",
    "    \"scale_120\": {\"scale\": 1.2},\n",
    "\n",
    "    \"noise_5_percent\": {\"add_noise\": True},\n",
    "    \"brightness_up\": {\"brightness\": 40},\n",
    "    \"brightness_down\": {\"brightness\": -40},\n",
    "\n",
    "    \"slow_motion_50_percent\": {\"slowdown_factor\": 2.0},\n",
    "    \"random_erase\": {\"random_erase\": True}\n",
    "}\n",
    "\n",
    "def augment_and_save_video(video_path, output_path, **kwargs):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    angle = kwargs.get(\"angle\", 0)\n",
    "    scale = kwargs.get(\"scale\", 1.0)\n",
    "    add_noise = kwargs.get(\"add_noise\", False)\n",
    "    brightness = kwargs.get(\"brightness\", 0)\n",
    "    slowdown_factor = kwargs.get(\"slowdown_factor\", 1.0)\n",
    "    random_erase = kwargs.get(\"random_erase\", False)\n",
    "\n",
    "    output_fps = max(1.0, original_fps / slowdown_factor)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, output_fps, (frame_width, frame_height))\n",
    "\n",
    "    center = (frame_width // 2, frame_height // 2)\n",
    "    rot_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        augmented_frame = frame.copy()\n",
    "\n",
    "        if angle != 0 or scale != 1.0:\n",
    "            augmented_frame = cv2.warpAffine(augmented_frame, rot_matrix, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "        if brightness != 0:\n",
    "            hsv = cv2.cvtColor(augmented_frame, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            v = np.clip(cv2.add(v, brightness), 0, 255)\n",
    "            final_hsv = cv2.merge((h, s, v))\n",
    "            augmented_frame = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, 13, augmented_frame.shape).astype(np.uint8)\n",
    "            augmented_frame = cv2.add(augmented_frame, noise)\n",
    "\n",
    "        if random_erase:\n",
    "            erase_h, erase_w = int(frame_height * 0.15), int(frame_width * 0.15)\n",
    "            x1 = np.random.randint(0, frame_width - erase_w)\n",
    "            y1 = np.random.randint(0, frame_height - erase_h)\n",
    "            augmented_frame[y1:y1+erase_h, x1:x1+erase_w] = (0, 0, 0) \n",
    "\n",
    "        out.write(augmented_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "\n",
    "def process_and_augment_videos():\n",
    "    print(\"--- Starting Full Video Augmentation Process ---\")\n",
    "\n",
    "    for action_name in ACTIONS_TO_PROCESS:\n",
    "        raw_data_dir = os.path.join(BASE_RAW_DATA_DIR, action_name)\n",
    "\n",
    "        if not os.path.isdir(raw_data_dir):\n",
    "            print(f\"\\nWarning: Directory not found for '{action_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        video_files = [f for f in os.listdir(raw_data_dir) if f.lower().endswith('.mp4')]\n",
    "        if not video_files:\n",
    "            print(f\"\\nInfo: No videos found for '{action_name}', skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing action: '{action_name}' ({len(video_files)} videos)\")\n",
    "\n",
    "        for video_name in tqdm(video_files, desc=f\"Augmenting '{action_name}'\"):\n",
    "            video_path = os.path.join(raw_data_dir, video_name)\n",
    "\n",
    "            for aug_name, aug_params in AUGMENTATION_TYPES.items():\n",
    "                output_dir = os.path.join(BASE_AUGMENTED_DATA_DIR, action_name, aug_name)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_path = os.path.join(output_dir, video_name)\n",
    "\n",
    "                augment_and_save_video(video_path, output_path, **aug_params)\n",
    "\n",
    "    print(f\"\\nFinished augmenting all specified video folders.\")\n",
    "    print(f\"Augmented videos saved in: {BASE_AUGMENTED_DATA_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_and_augment_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05b7cae",
   "metadata": {},
   "source": [
    "#### Block Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Full Video Augmentation Process ---\n",
      "\n",
      "Processing action: 'forearm_block' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'forearm_block': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [2:00:44<00:00,  9.06s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'high_guard' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'high_guard': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:51:31<00:00,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing action: 'parry' (800 videos)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmenting 'parry': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [1:52:02<00:00,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Finished augmenting all specified video folders.\n",
      "Augmented videos saved in: block_augmented_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "BASE_RAW_DATA_DIR = \"block_negative/block_dataset\"\n",
    "BASE_AUGMENTED_DATA_DIR = \"block_negative/block_augmented_data\"\n",
    "\n",
    "\n",
    "ACTIONS_TO_PROCESS = [\n",
    "    'forearm_block', 'high_guard', 'parry' , 'negative'\n",
    "]\n",
    "\n",
    "AUGMENTATION_TYPES = {\n",
    "    \"rot_pos15\": {\"angle\": 15},\n",
    "    \"rot_neg15\": {\"angle\": -15},\n",
    "    \"scale_80\": {\"scale\": 0.8},\n",
    "    \"scale_120\": {\"scale\": 1.2},\n",
    "\n",
    "    \"noise_5_percent\": {\"add_noise\": True},\n",
    "    \"brightness_up\": {\"brightness\": 40},\n",
    "    \"brightness_down\": {\"brightness\": -40},\n",
    "\n",
    "    \"slow_motion_50_percent\": {\"slowdown_factor\": 2.0},\n",
    "    \"random_erase\": {\"random_erase\": True}\n",
    "}\n",
    "\n",
    "def augment_and_save_video(video_path, output_path, **kwargs):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    angle = kwargs.get(\"angle\", 0)\n",
    "    scale = kwargs.get(\"scale\", 1.0)\n",
    "    add_noise = kwargs.get(\"add_noise\", False)\n",
    "    brightness = kwargs.get(\"brightness\", 0)\n",
    "    slowdown_factor = kwargs.get(\"slowdown_factor\", 1.0)\n",
    "    random_erase = kwargs.get(\"random_erase\", False)\n",
    "\n",
    "    output_fps = max(1.0, original_fps / slowdown_factor)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, output_fps, (frame_width, frame_height))\n",
    "\n",
    "    center = (frame_width // 2, frame_height // 2)\n",
    "    rot_matrix = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        augmented_frame = frame.copy()\n",
    "\n",
    "        if angle != 0 or scale != 1.0:\n",
    "            augmented_frame = cv2.warpAffine(augmented_frame, rot_matrix, (frame_width, frame_height))\n",
    "\n",
    "\n",
    "        if brightness != 0:\n",
    "            hsv = cv2.cvtColor(augmented_frame, cv2.COLOR_BGR2HSV)\n",
    "            h, s, v = cv2.split(hsv)\n",
    "            v = np.clip(cv2.add(v, brightness), 0, 255)\n",
    "            final_hsv = cv2.merge((h, s, v))\n",
    "            augmented_frame = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, 13, augmented_frame.shape).astype(np.uint8)\n",
    "            augmented_frame = cv2.add(augmented_frame, noise)\n",
    "\n",
    "        if random_erase:\n",
    "            erase_h, erase_w = int(frame_height * 0.15), int(frame_width * 0.15)\n",
    "            x1 = np.random.randint(0, frame_width - erase_w)\n",
    "            y1 = np.random.randint(0, frame_height - erase_h)\n",
    "            augmented_frame[y1:y1+erase_h, x1:x1+erase_w] = (0, 0, 0) \n",
    "\n",
    "        out.write(augmented_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "\n",
    "def process_and_augment_videos():\n",
    "    print(\"--- Starting Full Video Augmentation Process ---\")\n",
    "\n",
    "    for action_name in ACTIONS_TO_PROCESS:\n",
    "        raw_data_dir = os.path.join(BASE_RAW_DATA_DIR, action_name)\n",
    "\n",
    "        if not os.path.isdir(raw_data_dir):\n",
    "            print(f\"\\nWarning: Directory not found for '{action_name}', skipping.\")\n",
    "            continue\n",
    "\n",
    "        video_files = [f for f in os.listdir(raw_data_dir) if f.lower().endswith('.mp4')]\n",
    "        if not video_files:\n",
    "            print(f\"\\nInfo: No videos found for '{action_name}', skipping.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing action: '{action_name}' ({len(video_files)} videos)\")\n",
    "\n",
    "        for video_name in tqdm(video_files, desc=f\"Augmenting '{action_name}'\"):\n",
    "            video_path = os.path.join(raw_data_dir, video_name)\n",
    "\n",
    "            for aug_name, aug_params in AUGMENTATION_TYPES.items():\n",
    "                output_dir = os.path.join(BASE_AUGMENTED_DATA_DIR, action_name, aug_name)\n",
    "                os.makedirs(output_dir, exist_ok=True)\n",
    "                output_path = os.path.join(output_dir, video_name)\n",
    "\n",
    "                augment_and_save_video(video_path, output_path, **aug_params)\n",
    "\n",
    "    print(f\"\\nFinished augmenting all specified video folders.\")\n",
    "    print(f\"Augmented videos saved in: {BASE_AUGMENTED_DATA_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_and_augment_videos()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd6187",
   "metadata": {},
   "source": [
    "##### Mirroring Everthing (PUNCH + PUNCH_AUGUMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2f78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mirroring videos from 'combined_dataset' to 'mirrored_combined_dataset' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirroring combined_dataset:   0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirroring combined_dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [36:41<00:00,  1.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished mirroring dataset: combined_dataset\n",
      "\n",
      "--- Mirroring videos from 'augmented_data' to 'mirrored_augmented_data' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirroring augmented_data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36000/36000 [5:20:09<00:00,  1.87it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished mirroring dataset: augmented_data\n",
      "\n",
      "All mirroring processes complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "SOURCE_COMBINED_DIR = \"punch/combined_dataset\"\n",
    "SOURCE_AUGMENTED_DIR = \"punch/augmented_data\"\n",
    "\n",
    "TARGET_MIRRORED_COMBINED_DIR = \"punch/mirrored_combined_dataset\"\n",
    "TARGET_MIRRORED_AUGMENTED_DIR = \"punch/mirrored_augmented_data\"\n",
    "\n",
    "def mirror_and_save_video(video_path, output_path):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        mirrored_frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        out.write(mirrored_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "def mirror_dataset(source_base_dir, target_base_dir):\n",
    "\n",
    "    if not os.path.isdir(source_base_dir):\n",
    "        print(f\"Error: Source directory not found: {source_base_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Mirroring videos from '{source_base_dir}' to '{target_base_dir}' ---\")\n",
    "\n",
    "    video_paths = []\n",
    "    for root, _, files in os.walk(source_base_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.mp4'):\n",
    "                video_paths.append(os.path.join(root, file))\n",
    "\n",
    "    for video_path in tqdm(video_paths, desc=f\"Mirroring {os.path.basename(source_base_dir)}\"):\n",
    "        relative_path = os.path.relpath(video_path, source_base_dir)\n",
    "        output_path = os.path.join(target_base_dir, relative_path)\n",
    "        \n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        mirror_and_save_video(video_path, output_path)\n",
    "\n",
    "    print(f\"Finished mirroring dataset: {source_base_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mirror_dataset(SOURCE_COMBINED_DIR, TARGET_MIRRORED_COMBINED_DIR)\n",
    "    mirror_dataset(SOURCE_AUGMENTED_DIR, TARGET_MIRRORED_AUGMENTED_DIR)\n",
    "\n",
    "    print(\"\\nAll mirroring processes complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b897c",
   "metadata": {},
   "source": [
    "##### Mirroring Everthing (BLOCK + BLOCK_AUGUMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d251a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Mirroring videos from 'block_dataset' to 'mirrored_block_dataset' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirroring block_dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [23:59<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished mirroring dataset: block_dataset\n",
      "\n",
      "--- Mirroring videos from 'block_augmented_data' to 'mirrored_block_augmented_data' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirroring block_augmented_data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21600/21600 [3:36:55<00:00,  1.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Finished mirroring dataset: block_augmented_data\n",
      "\n",
      "All mirroring processes complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "SOURCE_COMBINED_DIR = \"block_negative/block_dataset\"\n",
    "SOURCE_AUGMENTED_DIR = \"block_negative/block_augmented_data\"\n",
    "\n",
    "TARGET_MIRRORED_COMBINED_DIR = \"block_negative/mirrored_block_dataset\"\n",
    "TARGET_MIRRORED_AUGMENTED_DIR = \"block_negative/mirrored_block_augmented_data\"\n",
    "\n",
    "def mirror_and_save_video(video_path, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Warning: Could not open video {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        mirrored_frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        out.write(mirrored_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "def mirror_dataset(source_base_dir, target_base_dir):\n",
    "    if not os.path.isdir(source_base_dir):\n",
    "        print(f\"Error: Source directory not found: {source_base_dir}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n--- Mirroring videos from '{source_base_dir}' to '{target_base_dir}' ---\")\n",
    "\n",
    "    video_paths = []\n",
    "    for root, _, files in os.walk(source_base_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.mp4'):\n",
    "                video_paths.append(os.path.join(root, file))\n",
    "\n",
    "    for video_path in tqdm(video_paths, desc=f\"Mirroring {os.path.basename(source_base_dir)}\"):\n",
    "        relative_path = os.path.relpath(video_path, source_base_dir)\n",
    "        output_path = os.path.join(target_base_dir, relative_path)\n",
    "        \n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        mirror_and_save_video(video_path, output_path)\n",
    "\n",
    "    print(f\"Finished mirroring dataset: {source_base_dir}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    mirror_dataset(SOURCE_COMBINED_DIR, TARGET_MIRRORED_COMBINED_DIR)\n",
    "    mirror_dataset(SOURCE_AUGMENTED_DIR, TARGET_MIRRORED_AUGMENTED_DIR)\n",
    "\n",
    "    print(\"\\nAll mirroring processes complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076bd63",
   "metadata": {},
   "source": [
    "#### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a39ab",
   "metadata": {},
   "source": [
    "##### Keypoints Without Mirroring (FOR PUNCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac0d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model: yolo11m-pose.pt...\n",
      "Model loaded on cuda.\n",
      "\n",
      "===== Processing RAW Data =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw 'jab':   0%|          | 0/800 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 175\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mâœ… All processing complete. Processed keypoints saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_NPY_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 125\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m video_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(action_input_dir, video_name)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Get the sequence of normalized keypoints\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m sequence \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video_to_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sequence \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sequence) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    128\u001b[0m     num_features \u001b[38;5;241m=\u001b[39m sequence[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[2], line 81\u001b[0m, in \u001b[0;36mprocess_video_to_sequence\u001b[0;34m(video_path, model)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Run YOLOv11 pose estimation\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Check if any person was detected\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeypoints\u001b[38;5;241m.\u001b[39mxy) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Get keypoints for the first detected person\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/ultralytics/engine/model.py:185\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    158\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    159\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    161\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/ultralytics/engine/model.py:555\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/ultralytics/engine/predictor.py:227\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/ultralytics/engine/predictor.py:308\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Warmup model\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtriton\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone_warmup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, [], \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/ultralytics/nn/autobackend.py:855\u001b[0m, in \u001b[0;36mAutoBackend.warmup\u001b[0;34m(self, imgsz)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwarmup\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgsz: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m)) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    849\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    Warm up the model by running one forward pass with a dummy input.\u001b[39;00m\n\u001b[1;32m    851\u001b[0m \n\u001b[1;32m    852\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;124;03m        imgsz (tuple): The shape of the dummy input tensor in the format (batch_size, channels, height, width)\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m  \u001b[38;5;66;03m# noqa (import here so torchvision import time not recorded in postprocess time)\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     warmup_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monnx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msaved_model, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(warmup_types) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtriton):\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/models/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mefficientnet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/models/convnext.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Permute\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstochastic_depth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StochasticDepth\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/ops/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgiou_loss\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m generalized_box_iou_loss\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Conv2dNormActivation, Conv3dNormActivation, FrozenBatchNorm2d, MLP, Permute, SqueezeExcitation\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoolers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiScaleRoIAlign\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_align, PSRoIAlign\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mps_roi_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ps_roi_pool, PSRoIPool\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/ops/poolers.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mboxes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m box_area\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mroi_align\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m roi_align\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# copying result_idx_in_level to a specific index in result[]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# is not supported by ONNX tracing yet.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# _onnx_merge_levels() is an implementation supported by ONNX\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# that merges the levels to the right indices\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39munused\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_onnx_merge_levels\u001b[39m(levels: Tensor, unmerged_results: List[Tensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torchvision/ops/roi_align.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn, Tensor\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_compile_supported\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BroadcastingList2\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pair\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/__init__.py:53\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph_break_reasons, guard_failures, orig_code_map, reset_frame_count\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Register polyfill functions\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolyfills\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loader \u001b[38;5;28;01mas\u001b[39;00m _  \u001b[38;5;66;03m# usort: skip # noqa: F401\u001b[39;00m\n\u001b[1;32m     56\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_in_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massume_constant_result\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     83\u001b[0m ]\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# allowlist this for weights_only load of NJTs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/polyfills/loader.py:25\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mPOLYFILLED_MODULE_NAMES\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/polyfills/loader.py:26\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See also the TYPE_CHECKING block in torch/_dynamo/polyfills/__init__.py\u001b[39;00m\n\u001b[1;32m     15\u001b[0m POLYFILLED_MODULE_NAMES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuiltins\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctools\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfx\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m POLYFILLED_MODULES: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModuleType\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m---> 26\u001b[0m     \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msubmodule\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolyfills\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m submodule \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULE_NAMES\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Unregister the builtin functions from _builtin_function_ids to let them to be\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# dispatched with the appropriate VariableTracker type. Otherwise, they will be\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# dispatched with BuiltinVariable if present in _builtin_function_ids.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m polyfill_module \u001b[38;5;129;01min\u001b[39;00m POLYFILLED_MODULES:\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/polyfills/builtins.py:31\u001b[0m\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124many\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menumerate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m ]\n\u001b[1;32m     27\u001b[0m _T \u001b[38;5;241m=\u001b[39m TypeVar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_T\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129;43m@substitute_in_graph\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_constant_fold_through\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/decorators.py:427\u001b[0m, in \u001b[0;36msubstitute_in_graph.<locals>.wrapper\u001b[0;34m(traceable_fn)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mid\u001b[39m(original_fn) \u001b[38;5;129;01min\u001b[39;00m _polyfilled_function_ids:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate polyfilled object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 427\u001b[0m rule_map: \u001b[38;5;28mdict\u001b[39m[Any, \u001b[38;5;28mtype\u001b[39m[VariableTracker]] \u001b[38;5;241m=\u001b[39m \u001b[43mget_torch_obj_rule_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_fn \u001b[38;5;129;01min\u001b[39;00m rule_map:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuplicate object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with different rules: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPolyfilledFunctionVariable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrule_map[original_fn]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2870\u001b[0m, in \u001b[0;36mget_torch_obj_rule_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2868\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m m\u001b[38;5;241m.\u001b[39mitems():  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   2869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k:\n\u001b[0;32m-> 2870\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mload_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2871\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _module_dir(torch) \u001b[38;5;241m+\u001b[39m k[\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/\u001b[39m\u001b[38;5;124m\"\u001b[39m) :]\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2901\u001b[0m, in \u001b[0;36mload_object\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2900\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid obj name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2901\u001b[0m         val \u001b[38;5;241m=\u001b[39m \u001b[43m_load_obj_from_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2902\u001b[0m     val \u001b[38;5;241m=\u001b[39m unwrap_if_wrapper(val)\n\u001b[1;32m   2903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_dynamo/trace_rules.py:2885\u001b[0m, in \u001b[0;36m_load_obj_from_str\u001b[0;34m(fully_qualified_name)\u001b[0m\n\u001b[1;32m   2883\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_obj_from_str\u001b[39m(fully_qualified_name):\n\u001b[1;32m   2884\u001b[0m     module, obj_name \u001b[38;5;241m=\u001b[39m fully_qualified_name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, maxsplit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m, obj_name)\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_higher_order_ops/map.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m suspend_functionalization\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maot_autograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AOTConfig, create_joint\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_higher_order_ops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     _has_potential_branch_input_alias,\n\u001b[1;32m      9\u001b[0m     _has_potential_branch_input_mutation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     UnsupportedAliasMutationException,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HigherOrderOperator\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:135\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraced_function_transforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     aot_dispatch_subclass,\n\u001b[1;32m    114\u001b[0m     create_functional_call,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m     fn_prepped_for_autograd,\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_aot_autograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     _get_autocast_states,\n\u001b[1;32m    123\u001b[0m     _get_symint_hints,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m     strict_zip,\n\u001b[1;32m    134\u001b[0m )\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartitioners\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_partition\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mzip\u001b[39m \u001b[38;5;241m=\u001b[39m strict_zip\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# This global counter increments every time we compile a graph with\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# AOTAutograd.  You can use this to correlate runtime error messages\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# with compile time (e.g., if you get an error at runtime saying\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# one counter is allocated per entire compiled block (but this block\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# may involve compiling multiple subgraphs; e.g., for forwards/backwards)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_functorch/partitioners.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CheckpointPolicy\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_info_provider\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphInfoProvider\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     39\u001b[0m     dp_knapsack,\n\u001b[1;32m     40\u001b[0m     greedy_knapsack,\n\u001b[1;32m     41\u001b[0m     ilp_knapsack,\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_activation_checkpointing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknapsack_evaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KnapsackEvaluator\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/torch/_functorch/_activation_checkpointing/graph_info_provider.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Graph, Node\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mGraphInfoProvider\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/networkx/__init__.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadwrite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Need to test with SciPy, when available\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m algorithms\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m linalg\n",
      "File \u001b[0;32m~/miniconda3/envs/boxingtf/lib/python3.10/site-packages/networkx/algorithms/__init__.py:52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msmetric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructuralholes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsifiers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummarization\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnetworkx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mswap\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1012\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:672\u001b[0m, in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_DIR = \"punch/combined_dataset\"\n",
    "AUGMENTED_DATA_DIR = \"punch/augmented_data\"\n",
    "BASE_NPY_DIR = \"punch/processed_keypoints\"\n",
    "MODEL_NAME = \"yolo11m-pose.pt\"\n",
    "\n",
    "ACTIONS_TO_PROCESS = ['jab', 'cross', 'hook', 'uppercut']\n",
    "SEQUENCE_LENGTH = 25\n",
    "KEYPOINT_INDICES = [5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "def normalize_keypoints(keypoints_xy):\n",
    "    left_shoulder = keypoints_xy[KEYPOINT_INDICES.index(5)]\n",
    "    right_shoulder = keypoints_xy[KEYPOINT_INDICES.index(6)]\n",
    "\n",
    "    if np.all(left_shoulder == 0) or np.all(right_shoulder == 0):\n",
    "        return None\n",
    "\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4:  \n",
    "        return None\n",
    "\n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "\n",
    "    normalized_kps = (keypoints_xy - center_point) / scale_dist\n",
    "    return normalized_kps.flatten() \n",
    "\n",
    "def process_video_to_sequence(video_path, model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "\n",
    "    frame_keypoints_list = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        if len(results) > 0 and len(results[0].keypoints.xy) > 0:\n",
    "            kps_xy = results[0].keypoints.xy[0].cpu().numpy()\n",
    "            \n",
    "            if kps_xy.shape[0] == 17:\n",
    "                relevant_kps = kps_xy[KEYPOINT_INDICES]\n",
    "                normalized = normalize_keypoints(relevant_kps)\n",
    "                if normalized is not None:\n",
    "                    frame_keypoints_list.append(normalized)\n",
    "    cap.release()\n",
    "    \n",
    "    return frame_keypoints_list\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading YOLO model: {MODEL_NAME}...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO(MODEL_NAME).to(device)\n",
    "    print(f\"Model loaded on {device}.\")\n",
    "\n",
    "    print(\"\\n===== Processing RAW Data =====\")\n",
    "    for action in ACTIONS_TO_PROCESS:\n",
    "        action_input_dir = os.path.join(RAW_DATA_DIR, action)\n",
    "        action_output_dir = os.path.join(BASE_NPY_DIR, action)\n",
    "        os.makedirs(action_output_dir, exist_ok=True)\n",
    "\n",
    "        if not os.path.isdir(action_input_dir): continue\n",
    "\n",
    "        videos = [v for v in os.listdir(action_input_dir) if v.endswith('.mp4')]\n",
    "        for video_name in tqdm(videos, desc=f\"Raw '{action}'\"):\n",
    "            video_path = os.path.join(action_input_dir, video_name)\n",
    "            \n",
    "            sequence = process_video_to_sequence(video_path, model)\n",
    "            \n",
    "            if sequence and len(sequence) > 0:\n",
    "                num_features = sequence[0].shape[0]\n",
    "                if len(sequence) < SEQUENCE_LENGTH:\n",
    "                    padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                    final_sequence = np.vstack((sequence, padding))\n",
    "                else:\n",
    "                    final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                \n",
    "                base_name = os.path.splitext(video_name)[0]\n",
    "                output_path = os.path.join(action_output_dir, f\"raw_{base_name}.npy\")\n",
    "                np.save(output_path, final_sequence)\n",
    "\n",
    "    print(\"\\n===== Processing AUGMENTED Data =====\")\n",
    "    for action in ACTIONS_TO_PROCESS:\n",
    "        action_input_dir = os.path.join(AUGMENTED_DATA_DIR, action)\n",
    "        action_output_dir = os.path.join(BASE_NPY_DIR, action)\n",
    "        os.makedirs(action_output_dir, exist_ok=True)\n",
    "\n",
    "        if not os.path.isdir(action_input_dir): continue\n",
    "        \n",
    "        for aug_type_folder in os.listdir(action_input_dir):\n",
    "            aug_folder_path = os.path.join(action_input_dir, aug_type_folder)\n",
    "            if not os.path.isdir(aug_folder_path): continue\n",
    "\n",
    "            videos = [v for v in os.listdir(aug_folder_path) if v.endswith('.mp4')]\n",
    "            for video_name in tqdm(videos, desc=f\"Aug '{action}/{aug_type_folder}'\"):\n",
    "                video_path = os.path.join(aug_folder_path, video_name)\n",
    "                \n",
    "                sequence = process_video_to_sequence(video_path, model)\n",
    "\n",
    "                if sequence and len(sequence) > 0:\n",
    "                    num_features = sequence[0].shape[0]\n",
    "                    if len(sequence) < SEQUENCE_LENGTH:\n",
    "                        padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                        final_sequence = np.vstack((sequence, padding))\n",
    "                    else:\n",
    "                        final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                    \n",
    "                    base_name = os.path.splitext(video_name)[0]\n",
    "                    output_path = os.path.join(action_output_dir, f\"aug_{aug_type_folder}_{base_name}.npy\")\n",
    "                    np.save(output_path, final_sequence)\n",
    "\n",
    "    print(f\"\\nAll processing complete. Processed keypoints saved in: {BASE_NPY_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328da99",
   "metadata": {},
   "source": [
    "##### Keypoints with mirroring (FOR PUNCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d7608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model: yolo11m-pose.pt...\n",
      "Model loaded on cuda.\n",
      "\n",
      "===== Processing Raw Data from 'combined_dataset' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw 'jab': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:33<00:00,  2.03it/s]\n",
      "Raw 'cross': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:41<00:00,  1.99it/s]\n",
      "Raw 'hook': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:40<00:00,  2.00it/s]\n",
      "Raw 'uppercut': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n",
      "Raw 'negative': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:47<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Augmented Data from 'augmented_data' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmented 'jab/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Augmented 'jab/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Augmented 'jab/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:45<00:00,  1.97it/s]\n",
      "Augmented 'jab/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'jab/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'jab/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.06it/s]\n",
      "Augmented 'jab/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.07it/s]\n",
      "Augmented 'jab/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'jab/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Augmented 'cross/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'cross/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'cross/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:56<00:00,  1.92it/s]\n",
      "Augmented 'cross/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'cross/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:37<00:00,  2.01it/s]\n",
      "Augmented 'cross/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:36<00:00,  2.02it/s]\n",
      "Augmented 'cross/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'cross/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:36<00:00,  2.02it/s]\n",
      "Augmented 'cross/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:39<00:00,  2.00it/s]\n",
      "Augmented 'hook/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'hook/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:36<00:00,  2.02it/s]\n",
      "Augmented 'hook/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:58<00:00,  1.91it/s]\n",
      "Augmented 'hook/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'hook/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:37<00:00,  2.01it/s]\n",
      "Augmented 'hook/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:35<00:00,  2.02it/s]\n",
      "Augmented 'hook/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'hook/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:37<00:00,  2.01it/s]\n",
      "Augmented 'hook/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:39<00:00,  2.00it/s]\n",
      "Augmented 'uppercut/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'uppercut/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Augmented 'uppercut/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:47<00:00,  1.96it/s]\n",
      "Augmented 'uppercut/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Augmented 'uppercut/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.06it/s]\n",
      "Augmented 'uppercut/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:24<00:00,  2.08it/s]\n",
      "Augmented 'uppercut/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.07it/s]\n",
      "Augmented 'uppercut/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:07<00:00,  2.17it/s]\n",
      "Augmented 'uppercut/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Augmented 'negative/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Augmented 'negative/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.06it/s]\n",
      "Augmented 'negative/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:45<00:00,  1.97it/s]\n",
      "Augmented 'negative/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Augmented 'negative/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Augmented 'negative/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'negative/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:33<00:00,  2.03it/s]\n",
      "Augmented 'negative/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:26<00:00,  2.07it/s]\n",
      "Augmented 'negative/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:46<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Mirrored Raw Data from 'mirrored_combined_dataset' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirrored Raw 'jab': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:17<00:00,  2.12it/s]\n",
      "Mirrored Raw 'cross': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:26<00:00,  2.07it/s]\n",
      "Mirrored Raw 'hook': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:22<00:00,  2.09it/s]\n",
      "Mirrored Raw 'uppercut': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:10<00:00,  2.16it/s]\n",
      "Mirrored Raw 'negative': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:25<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Mirrored Augmented Data from 'mirrored_augmented_data' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirrored Augmented 'jab/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:15<00:00,  2.13it/s]\n",
      "Mirrored Augmented 'jab/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Mirrored Augmented 'jab/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.06it/s]\n",
      "Mirrored Augmented 'jab/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Mirrored Augmented 'jab/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:12<00:00,  2.15it/s]\n",
      "Mirrored Augmented 'jab/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:21<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'jab/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:15<00:00,  2.13it/s]\n",
      "Mirrored Augmented 'jab/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:18<00:00,  2.11it/s]\n",
      "Mirrored Augmented 'jab/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:17<00:00,  2.12it/s]\n",
      "Mirrored Augmented 'cross/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:35<00:00,  2.02it/s]\n",
      "Mirrored Augmented 'cross/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Mirrored Augmented 'cross/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:00<00:00,  1.90it/s]\n",
      "Mirrored Augmented 'cross/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Mirrored Augmented 'cross/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:32<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'cross/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:33<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'cross/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Mirrored Augmented 'cross/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:35<00:00,  2.02it/s]\n",
      "Mirrored Augmented 'cross/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:32<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'hook/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:57<00:00,  1.91it/s]\n",
      "Mirrored Augmented 'hook/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'hook/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:32<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n",
      "Mirrored Augmented 'hook/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Mirrored Augmented 'uppercut/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:20<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'uppercut/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:24<00:00,  2.08it/s]\n",
      "Mirrored Augmented 'uppercut/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:45<00:00,  1.98it/s]\n",
      "Mirrored Augmented 'uppercut/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'uppercut/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'uppercut/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'uppercut/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:40<00:00,  2.00it/s]\n",
      "Mirrored Augmented 'uppercut/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:45<00:00,  1.97it/s]\n",
      "Mirrored Augmented 'uppercut/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:48<00:00,  1.96it/s]\n",
      "Mirrored Augmented 'negative/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:09<00:00,  1.86it/s]\n",
      "Mirrored Augmented 'negative/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:12<00:00,  1.85it/s]\n",
      "Mirrored Augmented 'negative/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:32<00:00,  1.77it/s]\n",
      "Mirrored Augmented 'negative/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:14<00:00,  1.84it/s]\n",
      "Mirrored Augmented 'negative/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:13<00:00,  1.85it/s]\n",
      "Mirrored Augmented 'negative/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:13<00:00,  1.85it/s]\n",
      "Mirrored Augmented 'negative/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:15<00:00,  1.84it/s]\n",
      "Mirrored Augmented 'negative/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:12<00:00,  1.85it/s]\n",
      "Mirrored Augmented 'negative/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [07:18<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All processing complete. Processed keypoints saved in: all_processed_keypoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_DIR = \"punch/combined_dataset\"\n",
    "AUGMENTED_DATA_DIR = \"punch/augmented_data\"\n",
    "MIRRORED_RAW_DATA_DIR = \"punch/mirrored_combined_dataset\"\n",
    "MIRRORED_AUGMENTED_DATA_DIR = \"punch/mirrored_augmented_data\"\n",
    "BASE_NPY_DIR = \"punch/processed_keypoints\"\n",
    "MODEL_NAME = \"yolo11m-pose.pt\"\n",
    "ACTIONS_TO_PROCESS = ['jab', 'cross', 'hook', 'uppercut']\n",
    "SEQUENCE_LENGTH = 25\n",
    "KEYPOINT_INDICES = [5, 6, 7, 8, 9, 10, 11, 12]  \n",
    "\n",
    "\n",
    "def normalize_keypoints(keypoints_xy):\n",
    "    left_shoulder = keypoints_xy[0]\n",
    "    right_shoulder = keypoints_xy[1]\n",
    "\n",
    "    if np.all(left_shoulder == 0) or np.all(right_shoulder == 0):\n",
    "        return None\n",
    "\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4:  \n",
    "        return None\n",
    "\n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "\n",
    "    normalized_kps = (keypoints_xy - center_point) / scale_dist\n",
    "    return normalized_kps.flatten() \n",
    "\n",
    "def process_video_to_sequence(video_path, model):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "\n",
    "    frame_keypoints_list = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        if len(results) > 0 and hasattr(results[0].keypoints, 'xy') and len(results[0].keypoints.xy) > 0:\n",
    "            kps_xy = results[0].keypoints.xy[0].cpu().numpy()\n",
    "            \n",
    "            if kps_xy.shape[0] == 17:\n",
    "                relevant_kps = kps_xy[KEYPOINT_INDICES]\n",
    "                normalized = normalize_keypoints(relevant_kps)\n",
    "                if normalized is not None:\n",
    "                    frame_keypoints_list.append(normalized)\n",
    "    cap.release()\n",
    "    \n",
    "    return frame_keypoints_list\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading YOLO model: {MODEL_NAME}...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO(MODEL_NAME).to(device)\n",
    "    print(f\"Model loaded on {device}.\")\n",
    "\n",
    "    all_data_sources = {\n",
    "        \"Raw\": (RAW_DATA_DIR, \"raw_\"),\n",
    "        \"Augmented\": (AUGMENTED_DATA_DIR, \"aug_\"),\n",
    "        \"Mirrored Raw\": (MIRRORED_RAW_DATA_DIR, \"mirrored_raw_\"),\n",
    "        \"Mirrored Augmented\": (MIRRORED_AUGMENTED_DATA_DIR, \"mirrored_aug_\")\n",
    "    }\n",
    "\n",
    "    for source_name, (source_dir, prefix) in all_data_sources.items():\n",
    "        print(f\"\\n===== Processing {source_name} Data from '{source_dir}' =====\")\n",
    "        if not os.path.isdir(source_dir):\n",
    "            print(f\"Directory not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        for action in ACTIONS_TO_PROCESS:\n",
    "            action_input_dir = os.path.join(source_dir, action)\n",
    "            action_output_dir = os.path.join(BASE_NPY_DIR, action)\n",
    "            os.makedirs(action_output_dir, exist_ok=True)\n",
    "\n",
    "            if not os.path.isdir(action_input_dir): continue\n",
    "\n",
    "            if \"Augmented\" in source_name:\n",
    "                for aug_type_folder in os.listdir(action_input_dir):\n",
    "                    aug_folder_path = os.path.join(action_input_dir, aug_type_folder)\n",
    "                    if not os.path.isdir(aug_folder_path): continue\n",
    "                    \n",
    "                    videos = [v for v in os.listdir(aug_folder_path) if v.endswith('.mp4')]\n",
    "                    desc = f\"{source_name} '{action}/{aug_type_folder}'\"\n",
    "                    \n",
    "                    for video_name in tqdm(videos, desc=desc):\n",
    "                        video_path = os.path.join(aug_folder_path, video_name)\n",
    "                        sequence = process_video_to_sequence(video_path, model)\n",
    "                        \n",
    "                        if sequence and len(sequence) > 0:\n",
    "                            num_features = sequence[0].shape[0]\n",
    "                            if len(sequence) < SEQUENCE_LENGTH:\n",
    "                                padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                                final_sequence = np.vstack((sequence, padding))\n",
    "                            else:\n",
    "                                final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                            \n",
    "                            base_name = os.path.splitext(video_name)[0]\n",
    "                            \n",
    "                            output_prefix = f\"{prefix}{aug_type_folder}_\"\n",
    "                            output_path = os.path.join(action_output_dir, f\"{output_prefix}{base_name}.npy\")\n",
    "                            np.save(output_path, final_sequence)\n",
    "            else: \n",
    "                videos = [v for v in os.listdir(action_input_dir) if v.endswith('.mp4')]\n",
    "                desc = f\"{source_name} '{action}'\"\n",
    "\n",
    "                for video_name in tqdm(videos, desc=desc):\n",
    "                    video_path = os.path.join(action_input_dir, video_name)\n",
    "                    sequence = process_video_to_sequence(video_path, model)\n",
    "                    \n",
    "                    if sequence and len(sequence) > 0:\n",
    "                        num_features = sequence[0].shape[0]\n",
    "                        if len(sequence) < SEQUENCE_LENGTH:\n",
    "                            padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                            final_sequence = np.vstack((sequence, padding))\n",
    "                        else:\n",
    "                            final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                        \n",
    "                        base_name = os.path.splitext(video_name)[0]\n",
    "                        output_path = os.path.join(action_output_dir, f\"{prefix}{base_name}.npy\")\n",
    "                        np.save(output_path, final_sequence)\n",
    "\n",
    "\n",
    "    print(f\"\\nAll processing complete. Processed keypoints saved in: {BASE_NPY_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99002053",
   "metadata": {},
   "source": [
    "#### KEY POINTS WITH MIRRORING (FOR BLOCK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20102624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model: yolo11m-pose.pt...\n",
      "Model loaded on cuda.\n",
      "\n",
      "===== Processing Raw Data from 'block_dataset' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Raw 'forearm_block': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:19<00:00,  2.11it/s]\n",
      "Raw 'high_guard': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:26<00:00,  2.07it/s]\n",
      "Raw 'parry': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:31<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Augmented Data from 'block_augmented_data' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Augmented 'forearm_block/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Augmented 'forearm_block/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Augmented 'forearm_block/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:50<00:00,  1.95it/s]\n",
      "Augmented 'forearm_block/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:36<00:00,  2.02it/s]\n",
      "Augmented 'forearm_block/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:33<00:00,  2.03it/s]\n",
      "Augmented 'forearm_block/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Augmented 'forearm_block/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:39<00:00,  2.00it/s]\n",
      "Augmented 'forearm_block/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Augmented 'forearm_block/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:38<00:00,  2.01it/s]\n",
      "Augmented 'high_guard/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:35<00:00,  2.02it/s]\n",
      "Augmented 'high_guard/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:18<00:00,  2.12it/s]\n",
      "Augmented 'high_guard/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:32<00:00,  2.04it/s]\n",
      "Augmented 'high_guard/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:16<00:00,  2.12it/s]\n",
      "Augmented 'high_guard/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Augmented 'high_guard/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:12<00:00,  2.15it/s]\n",
      "Augmented 'high_guard/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:12<00:00,  2.15it/s]\n",
      "Augmented 'high_guard/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:08<00:00,  2.17it/s]\n",
      "Augmented 'high_guard/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:07<00:00,  2.18it/s]\n",
      "Augmented 'parry/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:11<00:00,  2.15it/s]\n",
      "Augmented 'parry/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:14<00:00,  2.13it/s]\n",
      "Augmented 'parry/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.05it/s]\n",
      "Augmented 'parry/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Augmented 'parry/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:14<00:00,  2.13it/s]\n",
      "Augmented 'parry/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:12<00:00,  2.15it/s]\n",
      "Augmented 'parry/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:11<00:00,  2.16it/s]\n",
      "Augmented 'parry/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:13<00:00,  2.14it/s]\n",
      "Augmented 'parry/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:16<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Mirrored Raw Data from 'mirrored_block_dataset' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirrored Raw 'forearm_block': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:19<00:00,  2.11it/s]\n",
      "Mirrored Raw 'high_guard': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:16<00:00,  2.12it/s]\n",
      "Mirrored Raw 'parry': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:24<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Processing Mirrored Augmented Data from 'mirrored_block_augmented_data' =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mirrored Augmented 'forearm_block/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:22<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'forearm_block/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:22<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'forearm_block/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:34<00:00,  2.03it/s]\n",
      "Mirrored Augmented 'forearm_block/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:21<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'forearm_block/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:20<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'forearm_block/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:19<00:00,  2.11it/s]\n",
      "Mirrored Augmented 'forearm_block/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:19<00:00,  2.11it/s]\n",
      "Mirrored Augmented 'forearm_block/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:20<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'forearm_block/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:21<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'high_guard/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:18<00:00,  2.11it/s]\n",
      "Mirrored Augmented 'high_guard/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:21<00:00,  2.10it/s]\n",
      "Mirrored Augmented 'high_guard/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:40<00:00,  2.00it/s]\n",
      "Mirrored Augmented 'high_guard/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:23<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'high_guard/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:23<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'high_guard/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:23<00:00,  2.09it/s]\n",
      "Mirrored Augmented 'high_guard/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:25<00:00,  2.07it/s]\n",
      "Mirrored Augmented 'high_guard/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Mirrored Augmented 'high_guard/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:24<00:00,  2.08it/s]\n",
      "Mirrored Augmented 'parry/brightness_down': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:24<00:00,  2.08it/s]\n",
      "Mirrored Augmented 'parry/brightness_up': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:26<00:00,  2.07it/s]\n",
      "Mirrored Augmented 'parry/noise_5_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:45<00:00,  1.97it/s]\n",
      "Mirrored Augmented 'parry/random_erase': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Mirrored Augmented 'parry/rot_neg15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:28<00:00,  2.06it/s]\n",
      "Mirrored Augmented 'parry/rot_pos15': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'parry/scale_120': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:30<00:00,  2.05it/s]\n",
      "Mirrored Augmented 'parry/scale_80': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:27<00:00,  2.06it/s]\n",
      "Mirrored Augmented 'parry/slow_motion_50_percent': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 800/800 [06:29<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… All processing complete. Processed keypoints saved in: block_processed_keypoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "RAW_DATA_DIR = \"block_negative/block_dataset\"\n",
    "AUGMENTED_DATA_DIR = \"block_negative/block_augmented_data\"\n",
    "MIRRORED_RAW_DATA_DIR = \"block_negative/mirrored_block_dataset\"\n",
    "MIRRORED_AUGMENTED_DATA_DIR = \"block_negative/mirrored_block_augmented_data\"\n",
    "BASE_NPY_DIR = \"block_negative/processed_keypoints\"\n",
    "MODEL_NAME = \"yolo11m-pose.pt\"\n",
    "ACTIONS_TO_PROCESS = ['forearm_block', 'high_guard', 'parry', 'negative']\n",
    "SEQUENCE_LENGTH = 25\n",
    "KEYPOINT_INDICES = [5, 6, 7, 8, 9, 10, 11, 12]  \n",
    "\n",
    "\n",
    "def normalize_keypoints(keypoints_xy):\n",
    "    left_shoulder = keypoints_xy[0]\n",
    "    right_shoulder = keypoints_xy[1]\n",
    "\n",
    "    if np.all(left_shoulder == 0) or np.all(right_shoulder == 0):\n",
    "        return None\n",
    "\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4: \n",
    "        return None\n",
    "\n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "\n",
    "    normalized_kps = (keypoints_xy - center_point) / scale_dist\n",
    "    return normalized_kps.flatten() \n",
    "\n",
    "def process_video_to_sequence(video_path, model):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return None\n",
    "\n",
    "    frame_keypoints_list = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        results = model(frame, verbose=False)\n",
    "\n",
    "        if len(results) > 0 and hasattr(results[0].keypoints, 'xy') and len(results[0].keypoints.xy) > 0:\n",
    "            kps_xy = results[0].keypoints.xy[0].cpu().numpy()\n",
    "            \n",
    "            if kps_xy.shape[0] == 17:\n",
    "                relevant_kps = kps_xy[KEYPOINT_INDICES]\n",
    "                normalized = normalize_keypoints(relevant_kps)\n",
    "                if normalized is not None:\n",
    "                    frame_keypoints_list.append(normalized)\n",
    "    cap.release()\n",
    "    \n",
    "    return frame_keypoints_list\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading YOLO model: {MODEL_NAME}...\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = YOLO(MODEL_NAME).to(device)\n",
    "    print(f\"Model loaded on {device}.\")\n",
    "\n",
    "    all_data_sources = {\n",
    "        \"Raw\": (RAW_DATA_DIR, \"raw_\"),\n",
    "        \"Augmented\": (AUGMENTED_DATA_DIR, \"aug_\"),\n",
    "        \"Mirrored Raw\": (MIRRORED_RAW_DATA_DIR, \"mirrored_raw_\"),\n",
    "        \"Mirrored Augmented\": (MIRRORED_AUGMENTED_DATA_DIR, \"mirrored_aug_\")\n",
    "    }\n",
    "\n",
    "    for source_name, (source_dir, prefix) in all_data_sources.items():\n",
    "        print(f\"\\n===== Processing {source_name} Data from '{source_dir}' =====\")\n",
    "        if not os.path.isdir(source_dir):\n",
    "            print(f\"Directory not found, skipping.\")\n",
    "            continue\n",
    "\n",
    "        for action in ACTIONS_TO_PROCESS:\n",
    "            action_input_dir = os.path.join(source_dir, action)\n",
    "            action_output_dir = os.path.join(BASE_NPY_DIR, action)\n",
    "            os.makedirs(action_output_dir, exist_ok=True)\n",
    "\n",
    "            if not os.path.isdir(action_input_dir): continue\n",
    "\n",
    "            if \"Augmented\" in source_name:\n",
    "                for aug_type_folder in os.listdir(action_input_dir):\n",
    "                    aug_folder_path = os.path.join(action_input_dir, aug_type_folder)\n",
    "                    if not os.path.isdir(aug_folder_path): continue\n",
    "                    \n",
    "                    videos = [v for v in os.listdir(aug_folder_path) if v.endswith('.mp4')]\n",
    "                    desc = f\"{source_name} '{action}/{aug_type_folder}'\"\n",
    "                    \n",
    "                    for video_name in tqdm(videos, desc=desc):\n",
    "                        video_path = os.path.join(aug_folder_path, video_name)\n",
    "                        sequence = process_video_to_sequence(video_path, model)\n",
    "                        \n",
    "                        if sequence and len(sequence) > 0:\n",
    "                            num_features = sequence[0].shape[0]\n",
    "                            if len(sequence) < SEQUENCE_LENGTH:\n",
    "                                padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                                final_sequence = np.vstack((sequence, padding))\n",
    "                            else:\n",
    "                                final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                            \n",
    "                            base_name = os.path.splitext(video_name)[0]\n",
    "                            \n",
    "                            output_prefix = f\"{prefix}{aug_type_folder}_\"\n",
    "                            output_path = os.path.join(action_output_dir, f\"{output_prefix}{base_name}.npy\")\n",
    "                            np.save(output_path, final_sequence)\n",
    "            else: \n",
    "                videos = [v for v in os.listdir(action_input_dir) if v.endswith('.mp4')]\n",
    "                desc = f\"{source_name} '{action}'\"\n",
    "\n",
    "                for video_name in tqdm(videos, desc=desc):\n",
    "                    video_path = os.path.join(action_input_dir, video_name)\n",
    "                    sequence = process_video_to_sequence(video_path, model)\n",
    "                    \n",
    "                    if sequence and len(sequence) > 0:\n",
    "                        num_features = sequence[0].shape[0]\n",
    "                        if len(sequence) < SEQUENCE_LENGTH:\n",
    "                            padding = np.zeros((SEQUENCE_LENGTH - len(sequence), num_features))\n",
    "                            final_sequence = np.vstack((sequence, padding))\n",
    "                        else:\n",
    "                            final_sequence = np.array(sequence[:SEQUENCE_LENGTH])\n",
    "                        \n",
    "                        base_name = os.path.splitext(video_name)[0]\n",
    "                        output_path = os.path.join(action_output_dir, f\"{prefix}{base_name}.npy\")\n",
    "                        np.save(output_path, final_sequence)\n",
    "\n",
    "\n",
    "    print(f\"\\nAll processing complete. Processed keypoints saved in: {BASE_NPY_DIR}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1736ed",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896b1027",
   "metadata": {},
   "source": [
    "#### WITH MIRRORING FOR PUNCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data loaded: 51200 training samples, 12800 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:39<00:00, 40.08it/s, loss=0.2887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Validation Accuracy: 0.9641\n",
      "New best model saved with accuracy: 0.9641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.86it/s, loss=0.0852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Validation Accuracy: 0.9666\n",
      "New best model saved with accuracy: 0.9666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:23<00:00, 67.75it/s, loss=0.0619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Validation Accuracy: 0.9787\n",
      "New best model saved with accuracy: 0.9787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:23<00:00, 68.16it/s, loss=0.0497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Validation Accuracy: 0.9847\n",
      "New best model saved with accuracy: 0.9847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:24<00:00, 64.80it/s, loss=0.0401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Validation Accuracy: 0.9861\n",
      "New best model saved with accuracy: 0.9861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:25<00:00, 61.73it/s, loss=0.0332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Validation Accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:38<00:00, 41.78it/s, loss=0.0306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Validation Accuracy: 0.9898\n",
      "New best model saved with accuracy: 0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 53.27it/s, loss=0.0261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Validation Accuracy: 0.9880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 57.23it/s, loss=0.0265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Validation Accuracy: 0.9924\n",
      "New best model saved with accuracy: 0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.94it/s, loss=0.0215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Validation Accuracy: 0.9922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.60it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Validation Accuracy: 0.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 60.50it/s, loss=0.0190]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Validation Accuracy: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.92it/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Validation Accuracy: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.54it/s, loss=0.0161]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Validation Accuracy: 0.9941\n",
      "New best model saved with accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.92it/s, loss=0.0164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Validation Accuracy: 0.9945\n",
      "New best model saved with accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.61it/s, loss=0.0165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Validation Accuracy: 0.9962\n",
      "New best model saved with accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:24<00:00, 66.20it/s, loss=0.0149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Validation Accuracy: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:25<00:00, 61.71it/s, loss=0.0149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Validation Accuracy: 0.9924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 52.76it/s, loss=0.0132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Validation Accuracy: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 53.21it/s, loss=0.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Validation Accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.32it/s, loss=0.0108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Validation Accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 59.16it/s, loss=0.0142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Validation Accuracy: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.87it/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Validation Accuracy: 0.9940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 57.28it/s, loss=0.0126]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Validation Accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.27it/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Validation Accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.46it/s, loss=0.0136]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Validation Accuracy: 0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.28it/s, loss=0.0110]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Validation Accuracy: 0.9941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 59.21it/s, loss=0.0103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Validation Accuracy: 0.9954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.99it/s, loss=0.0120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Validation Accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.52it/s, loss=0.0113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Validation Accuracy: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.69it/s, loss=0.0102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Validation Accuracy: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.64it/s, loss=0.0099]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Validation Accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.42it/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Validation Accuracy: 0.9943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 59.09it/s, loss=0.0118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Validation Accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.41it/s, loss=0.0086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Validation Accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.36it/s, loss=0.0112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Validation Accuracy: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.94it/s, loss=0.0103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Validation Accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.38it/s, loss=0.0083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Validation Accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 59.17it/s, loss=0.0109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Validation Accuracy: 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 55.22it/s, loss=0.0103]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Validation Accuracy: 0.9972\n",
      "New best model saved with accuracy: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:23<00:00, 68.23it/s, loss=0.0112]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Validation Accuracy: 0.9974\n",
      "New best model saved with accuracy: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:23<00:00, 69.26it/s, loss=0.0063]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Validation Accuracy: 0.9969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:24<00:00, 66.01it/s, loss=0.0108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Validation Accuracy: 0.9972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:23<00:00, 68.66it/s, loss=0.0090]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Validation Accuracy: 0.9936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:25<00:00, 62.84it/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Validation Accuracy: 0.9968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 59.54it/s, loss=0.0083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Validation Accuracy: 0.9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:25<00:00, 63.79it/s, loss=0.0092]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Validation Accuracy: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.70it/s, loss=0.0107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Validation Accuracy: 0.9962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 55.04it/s, loss=0.0104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Validation Accuracy: 0.9953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 59.12it/s, loss=0.0080]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Validation Accuracy: 0.9959\n",
      "\n",
      "âœ… Training complete! Best model saved as 'punch_classifier.pth'\n",
      "\n",
      "===== Final Performance Report (from best epoch) =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         jab       1.00      1.00      1.00      3200\n",
      "       cross       1.00      1.00      1.00      3200\n",
      "        hook       1.00      1.00      1.00      3200\n",
      "    uppercut       1.00      1.00      1.00      3200\n",
      "\n",
      "    accuracy                           1.00     12800\n",
      "   macro avg       1.00      1.00      1.00     12800\n",
      "weighted avg       1.00      1.00      1.00     12800\n",
      "\n",
      "\n",
      "===== Confusion Matrix =====\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATA_DIR = \"punch/processed_keypoints\"\n",
    "ACTIONS = ['jab', 'cross', 'hook', 'uppercut']\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 25\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data_dir, actions):\n",
    "        self.data_dir = data_dir\n",
    "        self.actions = actions\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            action_path = os.path.join(data_dir, action)\n",
    "            if not os.path.isdir(action_path): continue\n",
    "\n",
    "            for filename in os.listdir(action_path):\n",
    "                if filename.endswith('.npy'):\n",
    "                    self.data.append(os.path.join(action_path, filename))\n",
    "                    self.labels.append(action_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data[idx]\n",
    "        sequence = np.load(path).astype(np.float32)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "class PunchClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(PunchClassifierLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = PoseDataset(DATA_DIR, ACTIONS)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset.labels\n",
    "    )\n",
    "    \n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "    \n",
    "    print(f\"Data loaded: {len(train_indices)} training samples, {len(val_indices)} validation samples.\")\n",
    "\n",
    "    sample_seq, _ = dataset[0]\n",
    "    input_size = sample_seq.shape[1]\n",
    "\n",
    "    model = PunchClassifierLSTM(\n",
    "        input_size=input_size, \n",
    "        hidden_size=128, \n",
    "        num_layers=2, \n",
    "        num_classes=len(ACTIONS)\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    final_val_preds, final_val_labels = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{running_loss / len(train_loader):.4f}'})\n",
    "        \n",
    "        model.eval()\n",
    "        epoch_val_preds, epoch_val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                epoch_val_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(epoch_val_labels, epoch_val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            final_val_preds = epoch_val_preds\n",
    "            final_val_labels = epoch_val_labels\n",
    "            torch.save(model.state_dict(), 'punch_classifier.pth')\n",
    "            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete! Best model saved as 'punch_classifier.pth'\")\n",
    "\n",
    "    print(\"\\n===== Final Performance Report (from best epoch) =====\")\n",
    "    report = classification_report(final_val_labels, final_val_preds, target_names=ACTIONS)\n",
    "    print(report)\n",
    "\n",
    "    print(\"\\n===== Confusion Matrix =====\")\n",
    "    cm = confusion_matrix(final_val_labels, final_val_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=ACTIONS, columns=ACTIONS)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2039c8",
   "metadata": {},
   "source": [
    "#### WITH MIRRORING FOR BLOCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ea25c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Data loaded: 51200 training samples, 12800 validation samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:37<00:00, 43.06it/s, loss=0.3545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Validation Accuracy: 0.9440\n",
      "New best model saved with accuracy: 0.9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 55.91it/s, loss=0.1238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 | Validation Accuracy: 0.9669\n",
      "New best model saved with accuracy: 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.25it/s, loss=0.0779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 | Validation Accuracy: 0.9795\n",
      "New best model saved with accuracy: 0.9795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.55it/s, loss=0.0646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 | Validation Accuracy: 0.9834\n",
      "New best model saved with accuracy: 0.9834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.26it/s, loss=0.0499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 | Validation Accuracy: 0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.51it/s, loss=0.0462]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 | Validation Accuracy: 0.9888\n",
      "New best model saved with accuracy: 0.9888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.29it/s, loss=0.0358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 | Validation Accuracy: 0.9886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.40it/s, loss=0.0366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 | Validation Accuracy: 0.9872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.61it/s, loss=0.0304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 | Validation Accuracy: 0.9895\n",
      "New best model saved with accuracy: 0.9895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.35it/s, loss=0.0288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 | Validation Accuracy: 0.9916\n",
      "New best model saved with accuracy: 0.9916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.94it/s, loss=0.0269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 | Validation Accuracy: 0.9903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.13it/s, loss=0.0251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 | Validation Accuracy: 0.9923\n",
      "New best model saved with accuracy: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.33it/s, loss=0.0240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 | Validation Accuracy: 0.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.23it/s, loss=0.0199]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 | Validation Accuracy: 0.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.14it/s, loss=0.0204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 | Validation Accuracy: 0.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 52.08it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 | Validation Accuracy: 0.9931\n",
      "New best model saved with accuracy: 0.9931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 53.60it/s, loss=0.0192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 | Validation Accuracy: 0.9934\n",
      "New best model saved with accuracy: 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 55.95it/s, loss=0.0184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 | Validation Accuracy: 0.9949\n",
      "New best model saved with accuracy: 0.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.44it/s, loss=0.0192]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 | Validation Accuracy: 0.9934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 52.02it/s, loss=0.0163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 | Validation Accuracy: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.58it/s, loss=0.0193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 | Validation Accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.31it/s, loss=0.0155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 | Validation Accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 53.40it/s, loss=0.0143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 | Validation Accuracy: 0.9950\n",
      "New best model saved with accuracy: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:27<00:00, 58.42it/s, loss=0.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 | Validation Accuracy: 0.9935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 60.22it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 | Validation Accuracy: 0.9952\n",
      "New best model saved with accuracy: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 61.31it/s, loss=0.0159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 | Validation Accuracy: 0.9954\n",
      "New best model saved with accuracy: 0.9954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:30<00:00, 52.45it/s, loss=0.0135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 | Validation Accuracy: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 56.00it/s, loss=0.0148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 | Validation Accuracy: 0.9923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:29<00:00, 54.86it/s, loss=0.0137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 | Validation Accuracy: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:31<00:00, 50.66it/s, loss=0.0147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 | Validation Accuracy: 0.9939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:31<00:00, 51.59it/s, loss=0.0129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 | Validation Accuracy: 0.9955\n",
      "New best model saved with accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:36<00:00, 43.87it/s, loss=0.0138]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 | Validation Accuracy: 0.9955\n",
      "New best model saved with accuracy: 0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:33<00:00, 47.99it/s, loss=0.0107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 | Validation Accuracy: 0.9915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:35<00:00, 45.26it/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 | Validation Accuracy: 0.9951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:38<00:00, 41.93it/s, loss=0.0137]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 | Validation Accuracy: 0.9938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:40<00:00, 39.92it/s, loss=0.0123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 | Validation Accuracy: 0.9959\n",
      "New best model saved with accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:28<00:00, 55.26it/s, loss=0.0135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 | Validation Accuracy: 0.9949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:38<00:00, 41.23it/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 | Validation Accuracy: 0.9933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:38<00:00, 41.10it/s, loss=0.0115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 | Validation Accuracy: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:38<00:00, 41.91it/s, loss=0.0152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 | Validation Accuracy: 0.9909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:37<00:00, 43.05it/s, loss=0.0117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 | Validation Accuracy: 0.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:35<00:00, 45.59it/s, loss=0.0128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 | Validation Accuracy: 0.9947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:33<00:00, 47.85it/s, loss=0.0119]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 | Validation Accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:32<00:00, 49.94it/s, loss=0.0111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 | Validation Accuracy: 0.9966\n",
      "New best model saved with accuracy: 0.9966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:33<00:00, 47.73it/s, loss=0.0120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 | Validation Accuracy: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:35<00:00, 45.16it/s, loss=0.0130]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 | Validation Accuracy: 0.9959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:36<00:00, 43.97it/s, loss=0.0096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 | Validation Accuracy: 0.9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:33<00:00, 47.84it/s, loss=0.0127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 | Validation Accuracy: 0.9956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:25<00:00, 62.52it/s, loss=0.0108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 | Validation Accuracy: 0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1600/1600 [00:26<00:00, 60.49it/s, loss=0.0125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 | Validation Accuracy: 0.9938\n",
      "\n",
      "âœ… Training complete! Best model saved as 'block_classifier.pth'\n",
      "\n",
      "===== Final Performance Report (from best epoch) =====\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "forearm_block       1.00      0.99      1.00      3200\n",
      "   high_guard       1.00      1.00      1.00      3200\n",
      "        parry       0.99      1.00      0.99      3200\n",
      "     negative       1.00      1.00      1.00      3200\n",
      "\n",
      "     accuracy                           1.00     12800\n",
      "    macro avg       1.00      1.00      1.00     12800\n",
      " weighted avg       1.00      1.00      1.00     12800\n",
      "\n",
      "\n",
      "===== Confusion Matrix =====\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "DATA_DIR = \"block_negative/processed_keypoints\"\n",
    "ACTIONS = ['forearm_block', 'high_guard', 'parry', 'negative']\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "SEQUENCE_LENGTH = 25\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "class PoseDataset(Dataset):\n",
    "    def __init__(self, data_dir, actions):\n",
    "        self.data_dir = data_dir\n",
    "        self.actions = actions\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "\n",
    "        for action_idx, action in enumerate(actions):\n",
    "            action_path = os.path.join(data_dir, action)\n",
    "            if not os.path.isdir(action_path): continue\n",
    "\n",
    "            for filename in os.listdir(action_path):\n",
    "                if filename.endswith('.npy'):\n",
    "                    self.data.append(os.path.join(action_path, filename))\n",
    "                    self.labels.append(action_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data[idx]\n",
    "        sequence = np.load(path).astype(np.float32)\n",
    "        label = self.labels[idx]\n",
    "        return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class BlockClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BlockClassifierLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    dataset = PoseDataset(DATA_DIR, ACTIONS)\n",
    "    train_indices, val_indices = train_test_split(\n",
    "        list(range(len(dataset))),\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=dataset.labels\n",
    "    )\n",
    "    \n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "    val_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
    "    \n",
    "    print(f\"Data loaded: {len(train_indices)} training samples, {len(val_indices)} validation samples.\")\n",
    "\n",
    "    sample_seq, _ = dataset[0]\n",
    "    input_size = sample_seq.shape[1]\n",
    "\n",
    "    model = BlockClassifierLSTM(\n",
    "        input_size=input_size, \n",
    "        hidden_size=128, \n",
    "        num_layers=2, \n",
    "        num_classes=len(ACTIONS)\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    final_val_preds, final_val_labels = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            train_pbar.set_postfix({'loss': f'{running_loss / len(train_loader):.4f}'})\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_preds, epoch_val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                epoch_val_preds.extend(predicted.cpu().numpy())\n",
    "                epoch_val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(epoch_val_labels, epoch_val_preds)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            final_val_preds = epoch_val_preds\n",
    "            final_val_labels = epoch_val_labels\n",
    "            torch.save(model.state_dict(), 'block_classifier.pth')\n",
    "            print(f\"New best model saved with accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining complete! Best model saved as 'block_classifier.pth'\")\n",
    "\n",
    "    print(\"\\n===== Final Performance Report (from best epoch) =====\")\n",
    "    report = classification_report(final_val_labels, final_val_preds, target_names=ACTIONS)\n",
    "    print(report)\n",
    "\n",
    "    print(\"\\n===== Confusion Matrix =====\")\n",
    "    cm = confusion_matrix(final_val_labels, final_val_preds)\n",
    "    cm_df = pd.DataFrame(cm, index=ACTIONS, columns=ACTIONS)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bb321",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3894f5dd",
   "metadata": {},
   "source": [
    "##### Only with punch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8809987a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "\n",
    "YOLO_MODEL_PATH = \"yolo11m-pose.pt\"\n",
    "LSTM_MODEL_PATH = \"punch_classifier.pth\"\n",
    "SEQUENCE_LENGTH = 25\n",
    "ACTIONS = ['jab', 'cross', 'hook', 'uppercut']\n",
    "KEYPOINT_INDICES = [5, 6, 7, 8, 9, 10, 11, 12]  \n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "COOLDOWN_FRAMES = 20\n",
    "PREVIEW_WIDTH = 1280\n",
    "PREVIEW_HEIGHT = 720\n",
    "L_WRIST, R_WRIST = 9, 10\n",
    "\n",
    "\n",
    "class PunchClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(PunchClassifierLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def normalize_keypoints(kps_xy):\n",
    "    left_shoulder, right_shoulder = kps_xy[0], kps_xy[1]\n",
    "    if np.all(left_shoulder == 0) or np.all(right_shoulder == 0): return None\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4: return None\n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "    normalized_kps = (kps_xy - center_point) / scale_dist\n",
    "    return normalized_kps.flatten()\n",
    "\n",
    "def detect_stance(keypoints):\n",
    "    left_shoulder, right_shoulder = keypoints[0], keypoints[1]\n",
    "    left_hip, right_hip = keypoints[6], keypoints[7]\n",
    "    if np.all(left_shoulder==0) or np.all(right_shoulder==0) or np.all(left_hip==0) or np.all(right_hip==0):\n",
    "        return \"Unknown\"\n",
    "    dist_left_side = np.linalg.norm(left_shoulder - left_hip)\n",
    "    dist_right_side = np.linalg.norm(right_shoulder - right_hip)\n",
    "    return 'Orthodox' if dist_left_side < dist_right_side else 'Southpaw'\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    yolo_model = YOLO(YOLO_MODEL_PATH).to(device)\n",
    "    input_size = len(KEYPOINT_INDICES) * 2\n",
    "    lstm_model = PunchClassifierLSTM(input_size, 128, 2, len(ACTIONS)).to(device)\n",
    "    lstm_model.load_state_dict(torch.load(LSTM_MODEL_PATH, map_location=device, weights_only=True))\n",
    "    lstm_model.eval()\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open camera. Please check the index (0, 1, 2, etc.) or connection.\")\n",
    "        return\n",
    "\n",
    "    player_data = {\n",
    "        1: {\"buffer\": deque(maxlen=SEQUENCE_LENGTH), \"cooldown\": 0, \"action\": \"\", \"count\": 0, \"stance\": \"\"},\n",
    "        2: {\"buffer\": deque(maxlen=SEQUENCE_LENGTH), \"cooldown\": 0, \"action\": \"\", \"count\": 0, \"stance\": \"\"}\n",
    "    }\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        yolo_results = yolo_model.track(frame, persist=True, verbose=False, tracker=\"bytetrack.yaml\")\n",
    "        \n",
    "        if yolo_results[0].boxes.id is None or yolo_results[0].keypoints is None:\n",
    "            display_frame = cv2.resize(frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "            cv2.imshow('Real-Time Boxing Analysis', display_frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "            continue\n",
    "\n",
    "        detected_people = []\n",
    "        keypoints_list = yolo_results[0].keypoints.xy.cpu().numpy()\n",
    "        boxes = yolo_results[0].boxes.xyxy.cpu().numpy()\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            center_x = (boxes[i][0] + boxes[i][2]) / 2 \n",
    "            detected_people.append({\"kps\": keypoints_list[i], \"center_x\": center_x})\n",
    "        \n",
    "        detected_people.sort(key=lambda p: p[\"center_x\"])\n",
    "\n",
    "        for i, person in enumerate(detected_people):\n",
    "            if i >= 2: break  \n",
    "            \n",
    "            player_id = i + 1 \n",
    "            \n",
    "            person_kps = person[\"kps\"]\n",
    "            relevant_kps = person_kps[KEYPOINT_INDICES]\n",
    "            \n",
    "            player_data[player_id][\"stance\"] = detect_stance(relevant_kps)\n",
    "\n",
    "            norm_kps = normalize_keypoints(relevant_kps)\n",
    "            if norm_kps is not None:\n",
    "                player_data[player_id][\"buffer\"].append(norm_kps)\n",
    "\n",
    "            if len(player_data[player_id][\"buffer\"]) == SEQUENCE_LENGTH and player_data[player_id][\"cooldown\"] == 0:\n",
    "                sequence_tensor = torch.tensor(np.array(player_data[player_id][\"buffer\"]), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = lstm_model(sequence_tensor)\n",
    "                    confidence, pred_idx = torch.max(torch.softmax(output, dim=1), 1)\n",
    "\n",
    "                if confidence.item() > CONFIDENCE_THRESHOLD:\n",
    "                    action = ACTIONS[pred_idx.item()]\n",
    "                    \n",
    "                    if action != 'negative':\n",
    "                        player_data[player_id][\"action\"] = action.upper()\n",
    "                        player_data[player_id][\"cooldown\"] = COOLDOWN_FRAMES\n",
    "                        player_data[player_id][\"count\"] += 1\n",
    "            \n",
    "            if player_data[player_id][\"cooldown\"] > 0:\n",
    "                player_data[player_id][\"cooldown\"] -= 1\n",
    "            if player_data[player_id][\"cooldown\"] == 0:\n",
    "                player_data[player_id][\"action\"] = \"\"\n",
    "\n",
    "        display_frame = frame.copy()\n",
    "        \n",
    "        p1_stance_text = f\"Player 1 ({player_data[1]['stance']})\"\n",
    "        p1_score_text = f\"Score: {player_data[1]['count']}\"\n",
    "        cv2.putText(display_frame, p1_stance_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(display_frame, p1_score_text, (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        if player_data[1][\"action\"]:\n",
    "            cv2.putText(display_frame, player_data[1][\"action\"], (10, 140), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 200, 100), 3, cv2.LINE_AA)\n",
    "\n",
    "        p2_stance_text = f\"Player 2 ({player_data[2]['stance']})\"\n",
    "        p2_score_text = f\"Score: {player_data[2]['count']}\"\n",
    "        (w_stance, _), _ = cv2.getTextSize(p2_stance_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        (w_score, _), _ = cv2.getTextSize(p2_score_text, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        cv2.putText(display_frame, p2_stance_text, (frame.shape[1] - w_stance - 10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(display_frame, p2_score_text, (frame.shape[1] - w_score - 10, 90), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        if player_data[2][\"action\"]:\n",
    "            (w_action, _), _ = cv2.getTextSize(player_data[2][\"action\"], cv2.FONT_HERSHEY_SIMPLEX, 1.5, 3)\n",
    "            cv2.putText(display_frame, player_data[2][\"action\"], (frame.shape[1] - w_action - 10, 140), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 200, 100), 3, cv2.LINE_AA)\n",
    "\n",
    "        resized_frame = cv2.resize(display_frame, (PREVIEW_WIDTH, PREVIEW_HEIGHT))\n",
    "        cv2.imshow('Real-Time Boxing Analysis', resized_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79080e74",
   "metadata": {},
   "source": [
    "##### With block, punch, hit&miss logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc621557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n",
      "âœ… All models loaded successfully.\n",
      "âœ… Script finished.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "import time\n",
    "\n",
    "\n",
    "config = {\n",
    "    #\"ROOT_DIR\": Path(__file__).resolve().parent,\n",
    "    \"ROOT_DIR\": Path.cwd(),\n",
    "    \"YOLO_MODEL_NAME\": \"yolo11m-pose.pt\",\n",
    "    \"PUNCH_LSTM_MODEL_NAME\": \"punch_classifier.pth\",\n",
    "    \"BLOCK_LSTM_MODEL_NAME\": \"block_classifier.pth\",\n",
    "    \"PUNCH_ACTIONS\": ['jab', 'cross', 'hook', 'uppercut'],\n",
    "    \"BLOCK_ACTIONS\": ['forearm_block', 'high_guard', 'parry', 'negative'],\n",
    "    \"KEYPOINT_MAP\": {\n",
    "        'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
    "        'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
    "        'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
    "        'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
    "    },\n",
    "\n",
    "    \"ACTION_KEYPOINTS\": [\n",
    "        'left_shoulder', 'right_shoulder', \n",
    "        'left_elbow', 'right_elbow', \n",
    "        'left_wrist', 'right_wrist',\n",
    "        'left_hip', 'right_hip'\n",
    "    ],\n",
    "\n",
    "    \"SEQUENCE_LENGTH\": 25,\n",
    "    \"CONFIDENCE_THRESHOLD\": 0.7,\n",
    "    \"KEYPOINT_CONF_THRESH\": 0.5,\n",
    "    \"PLAYER_TIMEOUT_FRAMES\": 150,\n",
    "    \"MIN_PUNCH_VELOCITY\": 1.0,\n",
    "    \"RECOVERY_FRAMES\": 15,\n",
    "    \"PREVIEW_WIDTH\": 1280,\n",
    "    \"PREVIEW_HEIGHT\": 720,\n",
    "    \"ACTION_TEXT_FADE_FRAMES\": 60,\n",
    "    \"PEAK_EXTENSION_THRESHOLD\": 0.1,\n",
    "    \"FPS_TARGET\": 30\n",
    "}\n",
    "\n",
    "class PlayerState(Enum):\n",
    "    IDLE = auto()\n",
    "    ATTACKING = auto()\n",
    "    BLOCKING = auto()\n",
    "    RECOVERING = auto()\n",
    "\n",
    "\n",
    "class ActionClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(ActionClassifierLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "def normalize_keypoints(kps, confs):\n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    ls_idx, rs_idx = k_map['left_shoulder'], k_map['right_shoulder']\n",
    "    \n",
    "    if confs[ls_idx] < config[\"KEYPOINT_CONF_THRESH\"] or confs[rs_idx] < config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "        return None\n",
    "    \n",
    "    left_shoulder, right_shoulder = kps[ls_idx], kps[rs_idx]\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4: \n",
    "        return None\n",
    "    \n",
    "    selected_indices = [k_map[name] for name in config[\"ACTION_KEYPOINTS\"]]\n",
    "    kps_subset = kps[selected_indices]\n",
    "    confs_subset = confs[selected_indices]\n",
    "    \n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "    normalized_kps = np.zeros_like(kps_subset)\n",
    "    valid_mask = confs_subset > config[\"KEYPOINT_CONF_THRESH\"]\n",
    "    normalized_kps[valid_mask] = (kps_subset[valid_mask] - center_point) / scale_dist\n",
    "    \n",
    "    return normalized_kps.flatten()\n",
    "\n",
    "def detect_stance(kps, confs):\n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    required_points = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "    if any(confs[k_map[name]] < config[\"KEYPOINT_CONF_THRESH\"] for name in required_points):\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    left_shoulder, right_shoulder = kps[k_map['left_shoulder']], kps[k_map['right_shoulder']]\n",
    "    left_hip, right_hip = kps[k_map['left_hip']], kps[k_map['right_hip']]\n",
    "    dist_left_side = np.linalg.norm(left_shoulder - left_hip)\n",
    "    dist_right_side = np.linalg.norm(right_shoulder - right_hip)\n",
    "    return 'Orthodox' if dist_left_side < dist_right_side else 'Southpaw'\n",
    "\n",
    "def calculate_limb_velocity(buffer, limb_idx, reference_idx, frame_interval):\n",
    "    if len(buffer) < frame_interval + 1:\n",
    "        return 0.0\n",
    "    \n",
    "    current_frame = buffer[-1]\n",
    "    prev_frame = buffer[-frame_interval-1]\n",
    "    \n",
    "    current_pos = current_frame[limb_idx]\n",
    "    prev_pos = prev_frame[reference_idx]\n",
    "    \n",
    "    if np.all(current_pos == 0) or np.all(prev_pos == 0):\n",
    "        return 0.0\n",
    "    \n",
    "    return np.linalg.norm(current_pos - prev_pos) / frame_interval\n",
    "\n",
    "def find_peak_extension_frame(buffer, wrist_idx, shoulder_idx):\n",
    "    max_extension = 0\n",
    "    peak_frame = -1\n",
    "    \n",
    "    for i, frame in enumerate(buffer):\n",
    "        wrist_pos = frame[wrist_idx]\n",
    "        shoulder_pos = frame[shoulder_idx]\n",
    "        \n",
    "        if np.all(wrist_pos == 0) or np.all(shoulder_pos == 0):\n",
    "            continue\n",
    "            \n",
    "        extension = np.linalg.norm(wrist_pos - shoulder_pos)\n",
    "        if extension > max_extension:\n",
    "            max_extension = extension\n",
    "            peak_frame = i\n",
    "    \n",
    "    return peak_frame if peak_frame != -1 else len(buffer) - 1\n",
    "\n",
    "def is_hit(attacker_buffer, defender_buffer, attacker_id, defender_id):\n",
    "    if len(attacker_buffer) < 5 or len(defender_buffer) < 1:\n",
    "        return False\n",
    "\n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    \n",
    "    wrist_deltas = []\n",
    "    if len(attacker_buffer) > 5:\n",
    "        start_frame = attacker_buffer[0][\"raw_kps\"]\n",
    "        end_frame = attacker_buffer[-1][\"raw_kps\"]\n",
    "        \n",
    "        left_wrist_delta = np.linalg.norm(\n",
    "            end_frame[k_map['left_wrist']] - start_frame[k_map['left_wrist']]\n",
    "        ) if (attacker_buffer[0][\"confs\"][k_map['left_wrist']] > config[\"KEYPOINT_CONF_THRESH\"] and\n",
    "               attacker_buffer[-1][\"confs\"][k_map['left_wrist']] > config[\"KEYPOINT_CONF_THRESH\"]) else 0\n",
    "               \n",
    "        right_wrist_delta = np.linalg.norm(\n",
    "            end_frame[k_map['right_wrist']] - start_frame[k_map['right_wrist']]\n",
    "        ) if (attacker_buffer[0][\"confs\"][k_map['right_wrist']] > config[\"KEYPOINT_CONF_THRESH\"] and\n",
    "               attacker_buffer[-1][\"confs\"][k_map['right_wrist']] > config[\"KEYPOINT_CONF_THRESH\"]) else 0\n",
    "        \n",
    "        wrist_idx = k_map['right_wrist'] if right_wrist_delta > left_wrist_delta else k_map['left_wrist']\n",
    "        shoulder_idx = k_map['right_shoulder'] if right_wrist_delta > left_wrist_delta else k_map['left_shoulder']\n",
    "    else:\n",
    "        wrist_idx = k_map['right_wrist']\n",
    "        shoulder_idx = k_map['right_shoulder']\n",
    "    \n",
    "    peak_idx = find_peak_extension_frame(\n",
    "        [f[\"raw_kps\"] for f in attacker_buffer],\n",
    "        wrist_idx,\n",
    "        shoulder_idx\n",
    "    )\n",
    "    \n",
    "    attacker_kps = attacker_buffer[peak_idx][\"raw_kps\"]\n",
    "    attacker_confs = attacker_buffer[peak_idx][\"confs\"]\n",
    "    \n",
    "    defender_kps = defender_buffer[-1][\"raw_kps\"]\n",
    "    defender_confs = defender_buffer[-1][\"confs\"]\n",
    "    \n",
    "    if peak_idx >= 2:\n",
    "        velocity = np.linalg.norm(\n",
    "            attacker_buffer[peak_idx][\"raw_kps\"][wrist_idx] - \n",
    "            attacker_buffer[peak_idx-2][\"raw_kps\"][wrist_idx]\n",
    "        ) / 2\n",
    "        if velocity < config[\"MIN_PUNCH_VELOCITY\"]:\n",
    "            return False\n",
    "\n",
    "    target_zones = {\n",
    "        \"head\": ['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear'],\n",
    "        \"torso\": ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "    }\n",
    "    \n",
    "    for zone_name, keypoint_names in target_zones.items():\n",
    "        zone_points = []\n",
    "        for name in keypoint_names:\n",
    "            idx = k_map.get(name)\n",
    "            if idx is None:\n",
    "                continue\n",
    "            if defender_confs[idx] > config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "                zone_points.append(defender_kps[idx])\n",
    "        \n",
    "        if len(zone_points) < 3:\n",
    "            continue\n",
    "        \n",
    "        hull = cv2.convexHull(np.array(zone_points, dtype=np.float32))\n",
    "        \n",
    "        for wrist_idx in [k_map['left_wrist'], k_map['right_wrist']]:\n",
    "            if attacker_confs[wrist_idx] > config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "                wrist_point = tuple(attacker_kps[wrist_idx].astype(np.float32))\n",
    "                if cv2.pointPolygonTest(hull, wrist_point, False) >= 0:\n",
    "                    return True\n",
    "    return False\n",
    "\n",
    "def update_player_data(detections, player_data, player_roles, last_seen, frame_counter):\n",
    "    for track_id, data in detections.items():\n",
    "        if track_id not in player_data:\n",
    "            if len(player_roles) >= 2:\n",
    "                continue\n",
    "            available_roles = {1, 2} - set(player_roles.values())\n",
    "            if not available_roles:\n",
    "                continue\n",
    "            role_id = min(available_roles)\n",
    "            player_roles[track_id] = role_id\n",
    "            player_data[track_id] = {\n",
    "                \"buffer\": deque(maxlen=config[\"SEQUENCE_LENGTH\"]),\n",
    "                \"state\": PlayerState.IDLE,\n",
    "                \"recovery_timer\": 0,\n",
    "                \"action_text\": \"\",\n",
    "                \"action_timer\": 0,\n",
    "                \"hits\": 0,\n",
    "                \"misses\": 0,\n",
    "                \"stance\": \"Unknown\",\n",
    "                \"last_action_time\": 0\n",
    "            }\n",
    "        \n",
    "        kps = data[\"kps\"]\n",
    "        confs = data[\"confs\"]\n",
    "        \n",
    "        norm_kps = normalize_keypoints(kps, confs)\n",
    "        if norm_kps is not None:\n",
    "            player_data[track_id][\"buffer\"].append({\n",
    "                \"norm_kps\": norm_kps,\n",
    "                \"raw_kps\": kps.copy(),\n",
    "                \"confs\": confs.copy(),\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "        \n",
    "        last_seen[track_id] = frame_counter\n",
    "\n",
    "def manage_player_timeouts(player_data, player_roles, last_seen, frame_counter):\n",
    "    timed_out_ids = [tid for tid, frame in last_seen.items() \n",
    "                    if frame_counter - frame > config[\"PLAYER_TIMEOUT_FRAMES\"]]\n",
    "    for track_id in timed_out_ids:\n",
    "        if track_id in player_data: \n",
    "            del player_data[track_id]\n",
    "        if track_id in last_seen: \n",
    "            del last_seen[track_id]\n",
    "        if track_id in player_roles: \n",
    "            del player_roles[track_id]\n",
    "\n",
    "def evaluate_and_update_states(player_data, player_roles, models, device):\n",
    "    tracked_ids = list(player_roles.keys())\n",
    "    if len(tracked_ids) < 2:\n",
    "        return\n",
    "\n",
    "    potential_actions = {}\n",
    "    for player_id in tracked_ids:\n",
    "        p_data = player_data[player_id]\n",
    "        if (p_data[\"state\"] == PlayerState.IDLE and \n",
    "            len(p_data[\"buffer\"]) == config[\"SEQUENCE_LENGTH\"]):\n",
    "\n",
    "            if len(p_data[\"buffer\"][0][\"norm_kps\"]) != 16:\n",
    "                continue\n",
    "            \n",
    "            sequence = np.array([f[\"norm_kps\"] for f in p_data[\"buffer\"]])\n",
    "            sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "               \n",
    "                punch_output = models['punch'](sequence_tensor)\n",
    "                punch_conf, punch_idx = torch.max(torch.softmax(punch_output, dim=1), 1)\n",
    "                \n",
    "                if punch_conf.item() > config[\"CONFIDENCE_THRESHOLD\"]:\n",
    "                    action_name = config[\"PUNCH_ACTIONS\"][punch_idx.item()]\n",
    "                    potential_actions[player_id] = {\n",
    "                        \"type\": \"punch\", \n",
    "                        \"name\": action_name,\n",
    "                        \"confidence\": punch_conf.item()\n",
    "                    }\n",
    "                else:\n",
    "                  \n",
    "                    block_output = models['block'](sequence_tensor)\n",
    "                    block_conf, block_idx = torch.max(torch.softmax(block_output, dim=1), 1)\n",
    "                    \n",
    "                    if block_conf.item() > config[\"CONFIDENCE_THRESHOLD\"]:\n",
    "                        action_name = config[\"BLOCK_ACTIONS\"][block_idx.item()]\n",
    "                        potential_actions[player_id] = {\n",
    "                            \"type\": \"block\", \n",
    "                            \"name\": action_name,\n",
    "                            \"confidence\": block_conf.item()\n",
    "                        }\n",
    "\n",
    "\n",
    "    p1_id, p2_id = tracked_ids[0], tracked_ids[1]\n",
    "    p1_action = potential_actions.get(p1_id)\n",
    "    p2_action = potential_actions.get(p2_id)\n",
    "    \n",
    "\n",
    "    if p1_action and p1_action[\"type\"] == \"punch\":\n",
    "        outcome = \"\"\n",
    "        if p2_action and p2_action[\"type\"] == \"block\":\n",
    "            outcome = \"BLOCKED\"\n",
    "        else:\n",
    "            if is_hit(\n",
    "                list(player_data[p1_id][\"buffer\"]), \n",
    "                list(player_data[p2_id][\"buffer\"]),\n",
    "                p1_id,\n",
    "                p2_id\n",
    "            ):\n",
    "                outcome = \"HIT\"\n",
    "                player_data[p1_id][\"hits\"] += 1\n",
    "            else:\n",
    "                outcome = \"MISS\"\n",
    "                player_data[p1_id][\"misses\"] += 1\n",
    "        \n",
    "        player_data[p1_id][\"action_text\"] = f'{p1_action[\"name\"].upper()} ({outcome})'\n",
    "        player_data[p1_id][\"action_timer\"] = config[\"ACTION_TEXT_FADE_FRAMES\"]\n",
    "        player_data[p1_id][\"state\"] = PlayerState.RECOVERING\n",
    "        player_data[p1_id][\"recovery_timer\"] = config[\"RECOVERY_FRAMES\"]\n",
    "        player_data[p1_id][\"last_action_time\"] = time.time()\n",
    "    \n",
    "    if p2_action and p2_action[\"type\"] == \"punch\":\n",
    "        outcome = \"\"\n",
    "        if p1_action and p1_action[\"type\"] == \"block\":\n",
    "            outcome = \"BLOCKED\"\n",
    "        else:\n",
    "            if is_hit(\n",
    "                list(player_data[p2_id][\"buffer\"]), \n",
    "                list(player_data[p1_id][\"buffer\"]),\n",
    "                p2_id,\n",
    "                p1_id\n",
    "            ):\n",
    "                outcome = \"HIT\"\n",
    "                player_data[p2_id][\"hits\"] += 1\n",
    "            else:\n",
    "                outcome = \"MISS\"\n",
    "                player_data[p2_id][\"misses\"] += 1\n",
    "        \n",
    "        player_data[p2_id][\"action_text\"] = f'{p2_action[\"name\"].upper()} ({outcome})'\n",
    "        player_data[p2_id][\"action_timer\"] = config[\"ACTION_TEXT_FADE_FRAMES\"]\n",
    "        player_data[p2_id][\"state\"] = PlayerState.RECOVERING\n",
    "        player_data[p2_id][\"recovery_timer\"] = config[\"RECOVERY_FRAMES\"]\n",
    "        player_data[p2_id][\"last_action_time\"] = time.time()\n",
    "    \n",
    "\n",
    "    for player_id, action in potential_actions.items():\n",
    "        if action[\"type\"] == \"block\":\n",
    "            player_data[player_id][\"state\"] = PlayerState.BLOCKING\n",
    "            player_data[player_id][\"action_text\"] = f'{action[\"name\"].upper()}'\n",
    "            player_data[player_id][\"action_timer\"] = config[\"ACTION_TEXT_FADE_FRAMES\"]\n",
    "            player_data[player_id][\"last_action_time\"] = time.time()\n",
    "\n",
    "\n",
    "def draw_ui(frame, player_data, player_roles, detections):\n",
    "    h, w, _ = frame.shape\n",
    "    hud_h = 110\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0, h - hud_h), (w, h), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)\n",
    "\n",
    "    for track_id, role_id in player_roles.items():\n",
    "        if track_id not in player_data or track_id not in detections:\n",
    "            continue\n",
    "            \n",
    "        p_data = player_data[track_id]\n",
    "        box = detections[track_id][\"box\"].astype(int)\n",
    "        kps = detections[track_id][\"kps\"]\n",
    "        confs = detections[track_id][\"confs\"]\n",
    "        \n",
    "        \n",
    "        color = (0, 255, 0)  \n",
    "        if p_data[\"state\"] == PlayerState.ATTACKING:\n",
    "            color = (0, 0, 255)  \n",
    "        elif p_data[\"state\"] == PlayerState.BLOCKING:\n",
    "            color = (255, 0, 0) \n",
    "        elif p_data[\"state\"] == PlayerState.RECOVERING:\n",
    "            color = (0, 255, 255)  \n",
    "\n",
    "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "        \n",
    "        cv2.putText(frame, f\"Player {role_id}\", (box[0], box[1] - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)    \n",
    "        \n",
    "\n",
    "        k_map = config[\"KEYPOINT_MAP\"]\n",
    "        target_zones = {\n",
    "            \"head\": (['nose', 'left_eye', 'right_eye', 'left_ear', 'right_ear'], (0, 255, 255)),  \n",
    "            \"torso\": (['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip'], (255, 0, 255))\n",
    "        }\n",
    "        \n",
    "        for zone_name, (keypoint_names, zone_color) in target_zones.items():\n",
    "            zone_points = []\n",
    "            for name in keypoint_names:\n",
    "                idx = k_map.get(name)\n",
    "                if idx is None:\n",
    "                    continue\n",
    "                if confs[idx] > config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "                    point = (int(kps[idx][0]), int(kps[idx][1]))\n",
    "                    zone_points.append(point)\n",
    "                    cv2.circle(frame, point, 5, zone_color, -1)\n",
    "            \n",
    "            if len(zone_points) >= 3:\n",
    "                hull = cv2.convexHull(np.array(zone_points, dtype=np.int32))\n",
    "                cv2.drawContours(frame, [hull], 0, zone_color, 2)\n",
    "                \n",
    "                if hull.size > 0:\n",
    "                    M = cv2.moments(hull)\n",
    "                    if M[\"m00\"] != 0:\n",
    "                        cX = int(M[\"m10\"] / M[\"m00\"])\n",
    "                        cY = int(M[\"m01\"] / M[\"m00\"])\n",
    "                        cv2.putText(frame, zone_name, (cX, cY), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, zone_color, 2)\n",
    "\n",
    "\n",
    "        if p_data[\"action_timer\"] > 0:\n",
    "            text_color = (100, 255, 100) if \"HIT\" in p_data[\"action_text\"] else \\\n",
    "                         (100, 100, 255) if \"MISS\" in p_data[\"action_text\"] else \\\n",
    "                         (255, 200, 100)\n",
    "            cv2.putText(frame, p_data[\"action_text\"], (box[0], box[1] - 40), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.2, text_color, 3, cv2.LINE_AA)\n",
    "        \n",
    "        x_pos = 50 if role_id == 1 else w - 450\n",
    "        stance_text = f\"Player {role_id} ({p_data.get('stance', 'N/A')})\"\n",
    "        cv2.putText(frame, stance_text, (x_pos, h - 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        score_text = f\"Score: {p_data['hits']} / {p_data['misses']}\"\n",
    "        cv2.putText(frame, score_text, (x_pos, h - 40), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        state_text = f\"State: {p_data['state'].name}\"\n",
    "        cv2.putText(frame, state_text, (x_pos + 250, h - 70), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (200, 200, 200), 2, cv2.LINE_AA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    try:\n",
    "        root = config[\"ROOT_DIR\"]\n",
    "        yolo_model = YOLO(root / config[\"YOLO_MODEL_NAME\"])\n",
    "        \n",
    "        input_size = len(config[\"ACTION_KEYPOINTS\"]) * 2\n",
    "        punch_model = ActionClassifierLSTM(input_size, 128, 2, len(config[\"PUNCH_ACTIONS\"])).to(device)\n",
    "        punch_model.load_state_dict(torch.load(root / config[\"PUNCH_LSTM_MODEL_NAME\"], map_location=device))\n",
    "        punch_model.eval()\n",
    "        \n",
    "        block_model = ActionClassifierLSTM(input_size, 128, 2, len(config[\"BLOCK_ACTIONS\"])).to(device)\n",
    "        block_model.load_state_dict(torch.load(root / config[\"BLOCK_LSTM_MODEL_NAME\"], map_location=device))\n",
    "        block_model.eval()\n",
    "        \n",
    "        models = {'punch': punch_model, 'block': block_model}\n",
    "        print(\"All models loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open camera.\")\n",
    "        return\n",
    "\n",
    "    player_data = {}\n",
    "    player_roles = {}\n",
    "    last_seen = {}\n",
    "    frame_counter = 0\n",
    "    last_time = time.time()\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_time\n",
    "        last_time = current_time\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        \n",
    "        frame_counter += 1\n",
    "        \n",
    "        yolo_results = yolo_model.track(\n",
    "            frame, \n",
    "            persist=True, \n",
    "            verbose=False, \n",
    "            tracker=\"bytetrack.yaml\",\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        current_detections = {}\n",
    "        if yolo_results and yolo_results[0].boxes and yolo_results[0].boxes.id is not None:\n",
    "            boxes = yolo_results[0].boxes.xyxy.cpu().numpy()\n",
    "            keypoints_list = yolo_results[0].keypoints.data.cpu().numpy()\n",
    "            track_ids = yolo_results[0].boxes.id.int().cpu().tolist()\n",
    "\n",
    "            for i, track_id in enumerate(track_ids):\n",
    "                current_detections[track_id] = {\n",
    "                    \"kps\": keypoints_list[i][:, :2],\n",
    "                    \"confs\": keypoints_list[i][:, 2],\n",
    "                    \"box\": boxes[i]\n",
    "                }\n",
    "        \n",
    "        update_player_data(current_detections, player_data, player_roles, last_seen, frame_counter)\n",
    "        manage_player_timeouts(player_data, player_roles, last_seen, frame_counter)\n",
    "        \n",
    "        for p_data in player_data.values():\n",
    "            if p_data[\"state\"] == PlayerState.RECOVERING:\n",
    "                p_data[\"recovery_timer\"] -= 1\n",
    "                if p_data[\"recovery_timer\"] <= 0:\n",
    "                    p_data[\"state\"] = PlayerState.IDLE\n",
    "            elif p_data[\"state\"] == PlayerState.BLOCKING:\n",
    "                if time.time() - p_data[\"last_action_time\"] > 0.5:  \n",
    "                    p_data[\"state\"] = PlayerState.IDLE\n",
    "        \n",
    "        evaluate_and_update_states(player_data, player_roles, models, device)\n",
    "        \n",
    "        for track_id, p_data in player_data.items():\n",
    "            if track_id in current_detections:\n",
    "                p_data['stance'] = detect_stance(\n",
    "                    current_detections[track_id]['kps'], \n",
    "                    current_detections[track_id]['confs']\n",
    "                )\n",
    "            if p_data[\"action_timer\"] > 0:\n",
    "                p_data[\"action_timer\"] -= 1\n",
    "        \n",
    "        display_frame = draw_ui(frame, player_data, player_roles, current_detections)\n",
    "        resized_frame = cv2.resize(display_frame, (config[\"PREVIEW_WIDTH\"], config[\"PREVIEW_HEIGHT\"]))\n",
    "        cv2.imshow('Boxing Analysis System', resized_frame)\n",
    "        \n",
    "        processing_time = time.time() - current_time\n",
    "        delay = max(1, int((1/config[\"FPS_TARGET\"] - processing_time) * 1000))\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"âœ… Script finished.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac7ec6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using device: cuda\n",
      "âœ… All models loaded successfully.\n",
      "âœ… Audio system initialized. Loaded 28 audio files.\n",
      "\n",
      "ðŸ“‹ Audio Files Summary:\n",
      "  - JAB: 4 files\n",
      "  - CROSS: 4 files\n",
      "  - HOOK: 3 files\n",
      "  - UPPERCUT: 4 files\n",
      "  - COMBO: 4 files\n",
      "  - ENCOURAGEMENT: 5 files\n",
      "  - TECHNIQUE: 4 files\n",
      "\n",
      "âœ… Shadow boxing session completed!\n",
      "ðŸ“Š Session Summary:\n",
      "   Duration: 00:13\n",
      "   Total Punches: 0\n",
      "   Max Combo: 0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "from enum import Enum, auto\n",
    "import time\n",
    "import threading\n",
    "import pygame  \n",
    "from queue import Queue\n",
    "import random\n",
    "import os\n",
    "\n",
    "config = {\n",
    "    \"ROOT_DIR\": Path.cwd(),\n",
    "    \"YOLO_MODEL_NAME\": \"yolo11m-pose.pt\",\n",
    "    \"PUNCH_LSTM_MODEL_NAME\": \"punch_classifier.pth\",\n",
    "    \"BLOCK_LSTM_MODEL_NAME\": \"block_classifier.pth\",\n",
    "    \"PUNCH_ACTIONS\": ['jab', 'cross', 'hook', 'uppercut'],\n",
    "    \"BLOCK_ACTIONS\": ['forearm_block', 'high_guard', 'parry', 'negative'],\n",
    "    \"KEYPOINT_MAP\": {\n",
    "        'nose': 0, 'left_eye': 1, 'right_eye': 2, 'left_ear': 3, 'right_ear': 4,\n",
    "        'left_shoulder': 5, 'right_shoulder': 6, 'left_elbow': 7, 'right_elbow': 8,\n",
    "        'left_wrist': 9, 'right_wrist': 10, 'left_hip': 11, 'right_hip': 12,\n",
    "        'left_knee': 13, 'right_knee': 14, 'left_ankle': 15, 'right_ankle': 16\n",
    "    },\n",
    "    \"ACTION_KEYPOINTS\": [\n",
    "        'left_shoulder', 'right_shoulder', \n",
    "        'left_elbow', 'right_elbow', \n",
    "        'left_wrist', 'right_wrist',\n",
    "        'left_hip', 'right_hip'\n",
    "    ],\n",
    "    \"SEQUENCE_LENGTH\": 25,\n",
    "    \"CONFIDENCE_THRESHOLD\": 0.7,\n",
    "    \"KEYPOINT_CONF_THRESH\": 0.5,\n",
    "    \"RECOVERY_FRAMES\": 15,\n",
    "    \"PREVIEW_WIDTH\": 1280,\n",
    "    \"PREVIEW_HEIGHT\": 720,\n",
    "    \"ACTION_TEXT_FADE_FRAMES\": 90,\n",
    "    \"FPS_TARGET\": 30,\n",
    "    \n",
    "    \"MIN_PUNCH_SPEED\": 2.0,\n",
    "    \"COMBO_TIMEOUT\": 3.0,  \n",
    "    \"AUDIO_ENABLED\": True,\n",
    "    \"ENCOURAGEMENT_FREQUENCY\": 5,  \n",
    "    \"TECHNIQUE_FEEDBACK\": True,\n",
    "\n",
    "    \"AUDIO_DIR\": \"audio\",  \n",
    "    \"AUDIO_VOLUME\": 1.0,\n",
    "    \"AUDIO_FORMAT\": [\".wav\", \".mp3\", \".ogg\"],\n",
    "      \"AUDIO_COOLDOWN\": 2.0\n",
    "}\n",
    "\n",
    "AUDIO_FILE_MAPPING = {\n",
    "    'jab': ['CLEAN JAB.wav','SHARP JAB.wav', 'GOOD JAB.wav','NICE JAB.wav'],\n",
    "    'cross': ['POWERFUL CROSS.wav','GOOD CROSS.wav','NICE CROSS.wav','STRONG CROSS.wav'],\n",
    "    'hook': ['SOLID HOOK.wav','GOOD HOOK.wav','NICE HOOK.wav'],\n",
    "    'uppercut': ['Solid uppercut.wav','Good uppercut.wav','Nice uppercut.wav','Strong uppercut.wav'],\n",
    "    #'combo': ['Great combo.wav','Nice combination.wav','Good flow.wav','Keep it up.wav'],\n",
    "    #'encouragement': ['Keep going.wav','Excellent form.wav','You\\'re doing great.wav','Nice work.wav','Good ryhthm.wav'],\n",
    "    #'technique': ['Keep your gaurd up.wav','nice form.wav','Good technique.wav','Stay balanced.wav']\n",
    "}\n",
    "\n",
    "\n",
    "class BoxerState(Enum):\n",
    "    IDLE = auto()\n",
    "    PUNCHING = auto()\n",
    "    RECOVERING = auto()\n",
    "\n",
    "\n",
    "class AudioFeedback:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            pygame.mixer.pre_init(frequency=44100, size=-16, channels=2, buffer=512)\n",
    "            pygame.mixer.init()\n",
    "            pygame.mixer.set_num_channels(8)  \n",
    "        except pygame.error as e:\n",
    "            print(f\"Pygame mixer could not be initialized: {e}. Audio will be disabled.\")\n",
    "            config[\"AUDIO_ENABLED\"] = False\n",
    "            return\n",
    "        \n",
    "        self.audio_queue = Queue()\n",
    "        self.audio_files = {}\n",
    "        self.audio_dir = Path(config[\"AUDIO_DIR\"])\n",
    "        self.last_played_time = 0\n",
    "        \n",
    "        self.punch_channel = pygame.mixer.Channel(0)\n",
    "\n",
    "        self._load_audio_files()\n",
    "        \n",
    "        if config[\"AUDIO_ENABLED\"]:\n",
    "            self.audio_thread = threading.Thread(target=self._audio_worker, daemon=True)\n",
    "            self.audio_thread.start()\n",
    "            \n",
    "            total_files = sum(len(files) for files in self.audio_files.values())\n",
    "            print(f\"Audio system initialized. Loaded {total_files} audio files.\")\n",
    "            self._print_loaded_files()\n",
    "\n",
    "    def _load_audio_files(self):\n",
    "        if not self.audio_dir.is_dir():\n",
    "            print(f\"Audio directory '{self.audio_dir}' not found. Audio will be disabled.\")\n",
    "            config[\"AUDIO_ENABLED\"] = False\n",
    "            return\n",
    "        \n",
    "        for category in AUDIO_FILE_MAPPING.keys():\n",
    "            self.audio_files[category] = []\n",
    "            \n",
    "        for category, filenames in AUDIO_FILE_MAPPING.items():\n",
    "            for filename in filenames:\n",
    "                file_path = self.audio_dir / filename\n",
    "                if file_path.exists():\n",
    "                    try:\n",
    "                        sound = pygame.mixer.Sound(str(file_path))\n",
    "                        sound.set_volume(config[\"AUDIO_VOLUME\"])\n",
    "                        self.audio_files[category].append(sound)\n",
    "                    except pygame.error as e:\n",
    "                        print(f\"Could not load '{file_path}': {e}\")\n",
    "                else:\n",
    "                    print(f\"File not found, skipping: {file_path}\")\n",
    "        \n",
    "        self._create_fallbacks()\n",
    "\n",
    "    def _create_fallbacks(self):\n",
    "        if not self.audio_files.get('combo'):\n",
    "            all_punch_sounds = []\n",
    "            for punch_type in ['jab', 'cross', 'hook', 'uppercut']:\n",
    "                all_punch_sounds.extend(self.audio_files.get(punch_type, []))\n",
    "            self.audio_files['combo'] = all_punch_sounds\n",
    "        \n",
    "        if not self.audio_files.get('encouragement'):\n",
    "            all_sounds = [s for cat_sounds in self.audio_files.values() for s in cat_sounds]\n",
    "            self.audio_files['encouragement'] = random.sample(all_sounds, min(len(all_sounds), 5))\n",
    "        \n",
    "        if not self.audio_files.get('technique'):\n",
    "            all_sounds = [s for cat_sounds in self.audio_files.values() for s in cat_sounds]\n",
    "            self.audio_files['technique'] = random.sample(all_sounds, min(len(all_sounds), 5))\n",
    "\n",
    "    def _print_loaded_files(self):\n",
    "        print(\"\\nAudio Files Summary:\")\n",
    "        for category, sounds in self.audio_files.items():\n",
    "            count = len(sounds)\n",
    "            if count > 0:\n",
    "                print(f\"  - {category.upper()}: {count} files\")\n",
    "            else:\n",
    "                print(f\"  - {category.upper()}: No files (will use fallbacks or skip)\")\n",
    "        print()\n",
    "\n",
    "    def _audio_worker(self):\n",
    "        while True:\n",
    "            category = self.audio_queue.get()\n",
    "            if category in self.audio_files and self.audio_files[category]:\n",
    "                try:\n",
    "                    sound = random.choice(self.audio_files[category])\n",
    "\n",
    "                    if category in config[\"PUNCH_ACTIONS\"]:\n",
    "                        self.punch_channel.play(sound)\n",
    "                    else:\n",
    "\n",
    "                        found_channel = None\n",
    "                        for i in range(1, pygame.mixer.get_num_channels()):\n",
    "                            ch = pygame.mixer.Channel(i)\n",
    "                            if not ch.get_busy():\n",
    "                                found_channel = ch\n",
    "                                break\n",
    "                        if found_channel:\n",
    "                            found_channel.play(sound)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error playing audio: {e}\")\n",
    "            self.audio_queue.task_done()\n",
    "\n",
    "    def play_audio(self, category):\n",
    "        current_time = time.time()\n",
    "        if (config[\"AUDIO_ENABLED\"] and \n",
    "            category in self.audio_files and\n",
    "            current_time - self.last_played_time > config[\"AUDIO_COOLDOWN\"]):\n",
    "            \n",
    "            if self.audio_files[category]:\n",
    "                while not self.audio_queue.empty():\n",
    "                    try:\n",
    "                        self.audio_queue.get_nowait()\n",
    "                    except queue.Empty:\n",
    "                        break\n",
    "                \n",
    "                self.audio_queue.put(category)\n",
    "                self.last_played_time = current_time\n",
    "\n",
    "    def set_volume(self, volume):\n",
    "        new_volume = max(0.0, min(1.0, volume))\n",
    "        config[\"AUDIO_VOLUME\"] = new_volume\n",
    "        for category_sounds in self.audio_files.values():\n",
    "            for sound in category_sounds:\n",
    "                sound.set_volume(new_volume)\n",
    "\n",
    "\n",
    "class ActionClassifierLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(ActionClassifierLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def normalize_keypoints(kps, confs):\n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    ls_idx, rs_idx = k_map['left_shoulder'], k_map['right_shoulder']\n",
    "    \n",
    "    if confs[ls_idx] < config[\"KEYPOINT_CONF_THRESH\"] or confs[rs_idx] < config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "        return None\n",
    "    \n",
    "    left_shoulder, right_shoulder = kps[ls_idx], kps[rs_idx]\n",
    "    scale_dist = np.linalg.norm(left_shoulder - right_shoulder)\n",
    "    if scale_dist < 1e-4: \n",
    "        return None\n",
    "    \n",
    "    selected_indices = [k_map[name] for name in config[\"ACTION_KEYPOINTS\"]]\n",
    "    kps_subset = kps[selected_indices]\n",
    "    confs_subset = confs[selected_indices]\n",
    "    \n",
    "    center_point = (left_shoulder + right_shoulder) / 2\n",
    "    normalized_kps = np.zeros_like(kps_subset)\n",
    "    valid_mask = confs_subset > config[\"KEYPOINT_CONF_THRESH\"]\n",
    "    normalized_kps[valid_mask] = (kps_subset[valid_mask] - center_point) / scale_dist\n",
    "    \n",
    "    return normalized_kps.flatten()\n",
    "\n",
    "def detect_stance(kps, confs):\n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    required_points = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "    if any(confs[k_map[name]] < config[\"KEYPOINT_CONF_THRESH\"] for name in required_points):\n",
    "        return \"Unknown\"\n",
    "        \n",
    "    left_shoulder, right_shoulder = kps[k_map['left_shoulder']], kps[k_map['right_shoulder']]\n",
    "    left_hip, right_hip = kps[k_map['left_hip']], kps[k_map['right_hip']]\n",
    "    dist_left_side = np.linalg.norm(left_shoulder - left_hip)\n",
    "    dist_right_side = np.linalg.norm(right_shoulder - right_hip)\n",
    "    return 'Orthodox' if dist_left_side < dist_right_side else 'Southpaw'\n",
    "\n",
    "def calculate_punch_speed(buffer):\n",
    "    if len(buffer) < 5:\n",
    "        return 0.0\n",
    "    \n",
    "    k_map = config[\"KEYPOINT_MAP\"]\n",
    "    recent_frames = list(buffer)[-5:]\n",
    "    \n",
    "    speeds = []\n",
    "    for wrist_key in ['left_wrist', 'right_wrist']:\n",
    "        wrist_idx = k_map[wrist_key]\n",
    "        positions = []\n",
    "        \n",
    "        for frame in recent_frames:\n",
    "            if frame[\"confs\"][wrist_idx] > config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "                positions.append(frame[\"raw_kps\"][wrist_idx])\n",
    "        \n",
    "        if len(positions) >= 2:\n",
    "            total_distance = sum(np.linalg.norm(positions[i+1] - positions[i]) \n",
    "                               for i in range(len(positions)-1))\n",
    "            speed = total_distance / len(positions)\n",
    "            speeds.append(speed)\n",
    "    \n",
    "    return max(speeds) if speeds else 0.0\n",
    "\n",
    "class ShadowBoxingAnalyzer:\n",
    "    def __init__(self, models, audio_feedback):\n",
    "        self.models = models\n",
    "        self.audio = audio_feedback\n",
    "        self.boxer_data = {\n",
    "            \"buffer\": deque(maxlen=config[\"SEQUENCE_LENGTH\"]),\n",
    "            \"state\": BoxerState.IDLE,\n",
    "            \"recovery_timer\": 0,\n",
    "            \"action_text\": \"\",\n",
    "            \"action_timer\": 0,\n",
    "            \"punch_count\": 0,\n",
    "            \"combo_count\": 0,\n",
    "            \"last_punch_time\": 0,\n",
    "            \"combo_history\": deque(maxlen=5),\n",
    "            \"stance\": \"Unknown\",\n",
    "            \"session_start\": time.time()\n",
    "        }\n",
    "    \n",
    "    def update_boxer_data(self, kps, confs):\n",
    "        norm_kps = normalize_keypoints(kps, confs)\n",
    "        if norm_kps is not None:\n",
    "            self.boxer_data[\"buffer\"].append({\n",
    "                \"norm_kps\": norm_kps,\n",
    "                \"raw_kps\": kps.copy(),\n",
    "                \"confs\": confs.copy(),\n",
    "                \"timestamp\": time.time()\n",
    "            })\n",
    "        \n",
    "        \n",
    "        self.boxer_data['stance'] = detect_stance(kps, confs)\n",
    "    \n",
    "    def analyze_action(self, device):\n",
    "        if (self.boxer_data[\"state\"] == BoxerState.IDLE and \n",
    "            len(self.boxer_data[\"buffer\"]) == config[\"SEQUENCE_LENGTH\"]):\n",
    "            \n",
    "            if len(self.boxer_data[\"buffer\"][0][\"norm_kps\"]) != 16:\n",
    "                return\n",
    "            \n",
    "            sequence = np.array([f[\"norm_kps\"] for f in self.boxer_data[\"buffer\"]])\n",
    "            sequence_tensor = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                punch_output = self.models['punch'](sequence_tensor)\n",
    "                punch_conf, punch_idx = torch.max(torch.softmax(punch_output, dim=1), 1)\n",
    "                \n",
    "                if punch_conf.item() > config[\"CONFIDENCE_THRESHOLD\"]:\n",
    "                    action_name = config[\"PUNCH_ACTIONS\"][punch_idx.item()]\n",
    "                    \n",
    "                    punch_speed = calculate_punch_speed(self.boxer_data[\"buffer\"])\n",
    "                    \n",
    "                    if punch_speed > config[\"MIN_PUNCH_SPEED\"]:\n",
    "                        self._handle_punch_detected(action_name, punch_conf.item())\n",
    "    \n",
    "    def _handle_punch_detected(self, punch_type, confidence):\n",
    "        current_time = time.time()\n",
    "        \n",
    "        self.boxer_data[\"punch_count\"] += 1\n",
    "        \n",
    "        is_combo = (current_time - self.boxer_data[\"last_punch_time\"]) < config[\"COMBO_TIMEOUT\"]\n",
    "        if is_combo:\n",
    "            self.boxer_data[\"combo_count\"] += 1\n",
    "        else:\n",
    "            self.boxer_data[\"combo_count\"] = 1\n",
    "        \n",
    "        self.boxer_data[\"last_punch_time\"] = current_time\n",
    "        self.boxer_data[\"combo_history\"].append(punch_type)\n",
    "        \n",
    "        quality_text = \"EXCELLENT\" if confidence > 0.9 else \"GOOD\" if confidence > 0.8 else \"NICE\"\n",
    "        self.boxer_data[\"action_text\"] = f'{quality_text} {punch_type.upper()}!'\n",
    "        self.boxer_data[\"action_timer\"] = config[\"ACTION_TEXT_FADE_FRAMES\"]\n",
    "        \n",
    "        self.boxer_data[\"state\"] = BoxerState.RECOVERING\n",
    "        self.boxer_data[\"recovery_timer\"] = config[\"RECOVERY_FRAMES\"]\n",
    "        \n",
    "        self._provide_audio_feedback(punch_type)\n",
    "    \n",
    "    def _provide_audio_feedback(self, punch_type):\n",
    "\n",
    "        if self.boxer_data[\"combo_count\"] >= 3:\n",
    "            self.audio.play_audio('combo')\n",
    "        elif self.boxer_data[\"punch_count\"] % config[\"ENCOURAGEMENT_FREQUENCY\"] == 0:\n",
    "            self.audio.play_audio('encouragement')\n",
    "        else:\n",
    "            self.audio.play_audio(punch_type)\n",
    "    \n",
    "    def update_state(self):\n",
    "        if self.boxer_data[\"state\"] == BoxerState.RECOVERING:\n",
    "            self.boxer_data[\"recovery_timer\"] -= 1\n",
    "            if self.boxer_data[\"recovery_timer\"] <= 0:\n",
    "                self.boxer_data[\"state\"] = BoxerState.IDLE\n",
    "\n",
    "        if self.boxer_data[\"action_timer\"] > 0:\n",
    "            self.boxer_data[\"action_timer\"] -= 1\n",
    "\n",
    "\n",
    "def draw_shadow_boxing_ui(frame, analyzer, detection=None):\n",
    "    h, w, _ = frame.shape\n",
    "    boxer_data = analyzer.boxer_data\n",
    "    \n",
    "    hud_h = 170\n",
    "    overlay = frame.copy()\n",
    "    cv2.rectangle(overlay, (0, h - hud_h), (w, h), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "    \n",
    "    if detection:\n",
    "        box = detection[\"box\"].astype(int)\n",
    "        kps = detection[\"kps\"]\n",
    "        confs = detection[\"confs\"]\n",
    "        \n",
    "        color = (0, 255, 0)  \n",
    "        if boxer_data[\"state\"] == BoxerState.PUNCHING:\n",
    "            color = (0, 0, 255)  \n",
    "        elif boxer_data[\"state\"] == BoxerState.RECOVERING:\n",
    "            color = (0, 255, 255)  \n",
    "        \n",
    "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "        \n",
    "\n",
    "        k_map = config[\"KEYPOINT_MAP\"]\n",
    "        important_points = ['left_wrist', 'right_wrist', 'left_elbow', 'right_elbow', \n",
    "                          'left_shoulder', 'right_shoulder']\n",
    "        \n",
    "        for point_name in important_points:\n",
    "            idx = k_map[point_name]\n",
    "            if confs[idx] > config[\"KEYPOINT_CONF_THRESH\"]:\n",
    "                point = (int(kps[idx][0]), int(kps[idx][1]))\n",
    "                cv2.circle(frame, point, 8, (0, 255, 255), -1)\n",
    "        \n",
    "        if boxer_data[\"action_timer\"] > 0:\n",
    "            text_color = (0, 255, 0)  \n",
    "            cv2.putText(frame, boxer_data[\"action_text\"], \n",
    "                       (box[0], box[1] - 40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1.5, text_color, 3, cv2.LINE_AA)\n",
    "    \n",
    "    session_time = int(time.time() - boxer_data[\"session_start\"])\n",
    "    minutes, seconds = divmod(session_time, 60)\n",
    "    \n",
    "    stats_y = h - 140\n",
    "    cv2.putText(frame, f\"Session Time: {minutes:02d}:{seconds:02d}\", \n",
    "                (20, stats_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, f\"Total Punches: {boxer_data['punch_count']}\", \n",
    "                (20, stats_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, f\"Current Combo: {boxer_data['combo_count']}\", \n",
    "                (20, stats_y + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.putText(frame, f\"Stance: {boxer_data['stance']}\", \n",
    "                (20, stats_y + 90), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    audio_status = \"ON\" if config[\"AUDIO_ENABLED\"] else \"OFF\"\n",
    "    audio_color = (0, 255, 0) if config[\"AUDIO_ENABLED\"] else (0, 0, 255)\n",
    "    cv2.putText(frame, f\"Audio: {audio_status} | Vol: {int(config['AUDIO_VOLUME']*100)}%\", \n",
    "                (20, stats_y + 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, audio_color, 2)\n",
    "\n",
    "    state_color = (0, 255, 0) if boxer_data[\"state\"] == BoxerState.IDLE else \\\n",
    "                  (0, 0, 255) if boxer_data[\"state\"] == BoxerState.PUNCHING else (0, 255, 255)\n",
    "    cv2.putText(frame, f\"State: {boxer_data['state'].name}\", \n",
    "                (w - 300, stats_y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, state_color, 2)\n",
    "\n",
    "    if boxer_data[\"combo_history\"]:\n",
    "        combo_text = \" â†’ \".join(list(boxer_data[\"combo_history\"])[-3:])  # Last 3 punches\n",
    "        cv2.putText(frame, f\"Recent: {combo_text}\", \n",
    "                    (w - 400, stats_y + 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "\n",
    "def main():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        root = config[\"ROOT_DIR\"]\n",
    "        yolo_model = YOLO(root / config[\"YOLO_MODEL_NAME\"])\n",
    "        \n",
    "        input_size = len(config[\"ACTION_KEYPOINTS\"]) * 2\n",
    "        punch_model = ActionClassifierLSTM(input_size, 128, 2, len(config[\"PUNCH_ACTIONS\"])).to(device)\n",
    "        punch_model.load_state_dict(torch.load(root / config[\"PUNCH_LSTM_MODEL_NAME\"], map_location=device))\n",
    "        punch_model.eval()\n",
    "        \n",
    "        block_model = ActionClassifierLSTM(input_size, 128, 2, len(config[\"BLOCK_ACTIONS\"])).to(device)\n",
    "        block_model.load_state_dict(torch.load(root / config[\"BLOCK_LSTM_MODEL_NAME\"], map_location=device))\n",
    "        block_model.eval()\n",
    "        \n",
    "        models = {'punch': punch_model, 'block': block_model}\n",
    "        print(\"All models loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading models: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio_feedback = AudioFeedback()\n",
    "    except Exception as e:\n",
    "        print(f\"Audio system failed to initialize: {e}\")\n",
    "        config[\"AUDIO_ENABLED\"] = False\n",
    "        audio_feedback = None\n",
    "        return\n",
    "    \n",
    "    analyzer = ShadowBoxingAnalyzer(models, audio_feedback)\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Cannot open camera.\")\n",
    "        return\n",
    "    \n",
    "    last_time = time.time()\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_time\n",
    "        last_time = current_time\n",
    "        \n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            break\n",
    "        \n",
    "        yolo_results = yolo_model(frame, verbose=False, device=device)\n",
    "        \n",
    "        current_detection = None\n",
    "        if yolo_results and yolo_results[0].boxes is not None and len(yolo_results[0].boxes) > 0:\n",
    "            boxes = yolo_results[0].boxes.xyxy.cpu().numpy()\n",
    "            keypoints_list = yolo_results[0].keypoints.data.cpu().numpy()\n",
    "            \n",
    "            if len(boxes) > 0 and len(keypoints_list) > 0:\n",
    "                current_detection = {\n",
    "                    \"kps\": keypoints_list[0][:, :2],\n",
    "                    \"confs\": keypoints_list[0][:, 2],\n",
    "                    \"box\": boxes[0]\n",
    "                }\n",
    "                \n",
    "                analyzer.update_boxer_data(current_detection[\"kps\"], current_detection[\"confs\"])\n",
    "        \n",
    "        analyzer.analyze_action(device)\n",
    "        \n",
    "        analyzer.update_state()\n",
    "        \n",
    "        display_frame = draw_shadow_boxing_ui(frame, analyzer, current_detection)\n",
    "        resized_frame = cv2.resize(display_frame, (config[\"PREVIEW_WIDTH\"], config[\"PREVIEW_HEIGHT\"]))\n",
    "        cv2.imshow('Shadow Boxing Trainer', resized_frame)\n",
    "        \n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            config[\"AUDIO_ENABLED\"] = not config[\"AUDIO_ENABLED\"]\n",
    "            status = \"enabled\" if config[\"AUDIO_ENABLED\"] else \"disabled\"\n",
    "            print(f\"Audio feedback {status}\")\n",
    "        elif key == ord('+') or key == ord('='):\n",
    "            new_volume = min(1.0, config[\"AUDIO_VOLUME\"] + 0.1)\n",
    "            audio_feedback.set_volume(new_volume)\n",
    "            print(f\"Volume: {int(new_volume*100)}%\")\n",
    "        elif key == ord('-'):\n",
    "            new_volume = max(0.0, config[\"AUDIO_VOLUME\"] - 0.1)\n",
    "            audio_feedback.set_volume(new_volume)\n",
    "            print(f\"Volume: {int(new_volume*100)}%\")\n",
    "        \n",
    "        processing_time = time.time() - current_time\n",
    "        delay = max(1, int((1/config[\"FPS_TARGET\"] - processing_time) * 1000))\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pygame.mixer.quit()\n",
    "    print(\"Shadow boxing session completed!\")\n",
    "    \n",
    "    total_time = int(time.time() - analyzer.boxer_data[\"session_start\"])\n",
    "    minutes, seconds = divmod(total_time, 60)\n",
    "    print(f\"Session Summary:\")\n",
    "    print(f\"   Duration: {minutes:02d}:{seconds:02d}\")\n",
    "    print(f\"   Total Punches: {analyzer.boxer_data['punch_count']}\")\n",
    "    print(f\"   Max Combo: {max(analyzer.boxer_data.get('max_combo', 0), analyzer.boxer_data['combo_count'])}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb20226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boxingtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
